{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Dependencies\n",
    "import torch # PyToch library for building and training neural networks\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "import numpy as np # for numerical calculations\n",
    "from collections import namedtuple, deque # provides useful data structures may not need\n",
    "import random # for random sampling \n",
    "from mss import mss # for grabbing a screen shot of a monitor \n",
    "import pydirectinput # for mouse and keyboard input on windows\n",
    "import cv2 as cv # for image and video processing\n",
    "import pytesseract # OCR tool for reading text from images\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import time\n",
    "from gymnasium import Env\n",
    "from gymnasium.spaces import Box, Discrete\n",
    "from gymnasium.utils import env_checker  # Import the environment checker\n",
    "from collections import deque\n",
    "\n",
    "def plot_learning_curve(x, scores, epsilons, filename, lines=None):\n",
    "\tfig=plt.figure()\n",
    "\tax=fig.add_subplot(111, label=\"1\")\n",
    "\tax2=fig.add_subplot(111, label=\"2\", frame_on=False)\n",
    "\n",
    "\tax.plot(x, epsilons, color=\"C0\")\n",
    "\tax.set_xlabel(\"Training Steps\", color=\"C0\")\n",
    "\tax.set_ylabel(\"Epsilon\", color=\"C0\")\n",
    "\tax.tick_params(axis='x', colors=\"C0\")\n",
    "\tax.tick_params(axis='y', colors=\"C0\")\n",
    "\n",
    "\tN = len(scores)\n",
    "\trunning_avg = np.empty(N)\n",
    "\tfor t in range(N):\n",
    "\t\trunning_avg[t] = np.mean(scores[max(0, t-20):(t+1)])\n",
    "\n",
    "\tax2.scatter(x, running_avg, color=\"C1\")\n",
    "\tax2.axes.get_xaxis().set_visible(False)\n",
    "\tax2.yaxis.tick_right()\n",
    "\tax2.set_ylabel('Score', color=\"C1\")\n",
    "\tax2.yaxis.set_label_position('right')\n",
    "\tax2.tick_params(axis='y', colors=\"C1\")\n",
    "\n",
    "\tif lines is not None:\n",
    "\t\tfor line in lines:\n",
    "\t\t\tplt.axvline(x=line)\n",
    "\n",
    "\tplt.savefig(filename)\n",
    "\n",
    "\n",
    "# Designing DQN Model\n",
    "class DQN(nn.Module): # defines a new neural network model that inherits from Pytorch's base class nn.module\n",
    "\tdef __init__(self, lr, input_dims, fc1_dims, fc2_dims, num_actions): \n",
    "\t\tsuper(DQN, self).__init__() # calls the initializer of the parent class nn.module \n",
    "\t\tself.input_dims = input_dims\n",
    "\t\tself.fc1_dims = fc1_dims\n",
    "\t\tself.fc2_dims = fc2_dims\n",
    "\t\tself.conv1 = nn.Conv2d(self.input_dims[0], 32, kernel_size=8, stride=4) # convolutional layer with 32 filters, each of size 8 x8, applied with a stride of 4\n",
    "\t\tself.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2) # convolutional layer with 64 filters, each of size 4 x 4, applied with a stride of 2\n",
    "\t\tself.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1) # convolutional layer with 64 filters, each of size 3 x 3\n",
    "\t\tself.fc_input_size = self._calculate_fc_input_size(self.input_dims)\n",
    "\t\tself.fc1 = nn.Linear(self.fc_input_size, self.fc1_dims) # fully connected layer with 512 units\n",
    "\t\tself.fc2 = nn.Linear(self.fc2_dims, num_actions) # final fully connected layer with output units equal to the number of possible actions\n",
    "\t\tself.optimizer = Adam(self.parameters(), lr=lr)\n",
    "\t\tself.loss = nn.MSELoss()\n",
    "\t\tself.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\t\tself.to(self.device)\n",
    "\t\t\n",
    "\tdef _calculate_fc_input_size(self, input_dims):\n",
    "\t\twith torch.no_grad():\n",
    "\t\t\tdummy_input = torch.zeros(1, input_dims[0], 50, 80)\n",
    "\t\t\tx = torch.relu(self.conv1(dummy_input))\n",
    "\t\t\tx = torch.relu(self.conv2(x))\n",
    "\t\t\tx = torch.relu(self.conv3(x))\n",
    "\t\t\t\n",
    "\t\t\treturn x.view(1, -1).size(1)  # Flatten and get the size\n",
    "\t\t\n",
    "\tdef forward(self, x):\n",
    "\t\tx = torch.relu(self.conv1(x))\n",
    "\t\tx = torch.relu(self.conv2(x))\n",
    "\t\tx = torch.relu(self.conv3(x))\n",
    "\t\tx = x.view(x.size(0), -1) # Flatten the output from conv layers\n",
    "\t\tx = torch.relu(self.fc1(x))\n",
    "\t\tactions =  self.fc2(x)  # Output Q-Values for each action\n",
    "\t\t\n",
    "\t\treturn actions\n",
    "\t\n",
    "# Creating Replay Buffer\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, max_mem_size, input_dims):\n",
    "        self.mem_size = max_mem_size \n",
    "        self.mem_cntr = 0\n",
    "        self.state_memory = np.zeros((self.mem_size, *input_dims), dtype=np.float32)\n",
    "        self.new_state_memory = np.zeros((self.mem_size, *input_dims), dtype=np.float32)\n",
    "        self.action_memory = np.zeros(self.mem_size, dtype=np.int32)\n",
    "        self.reward_memory = np.zeros(self.mem_size, dtype=np.float32)\n",
    "        self.terminal_memory = np.zeros(self.mem_size, dtype=np.bool_)\n",
    "    def store_transition(self, state, action, reward, state_, done):\n",
    "        index = self.mem_cntr % self.mem_size\n",
    "        self.state_memory[index] = state\n",
    "        self.new_state_memory[index] = state_\n",
    "        self.reward_memory[index] = reward\n",
    "        self.action_memory[index] = action\n",
    "        self.terminal_memory[index] = done\n",
    "        self.mem_cntr += 1\n",
    "    def sample_buffer(self, batch_size):\n",
    "        max_mem = min(self.mem_cntr, self.mem_size)\n",
    "        batch =random.sample(range(max_mem), batch_size)\n",
    "        states = self.state_memory[batch]\n",
    "        actions = self.action_memory[batch]\n",
    "        rewards = self.reward_memory[batch]\n",
    "        new_states = self.new_state_memory[batch]\n",
    "        dones = self.terminal_memory[batch]\n",
    "        return states, actions, rewards, new_states, dones\n",
    "  \n",
    "# Creating DQN Agent\n",
    "class DQNAgent:       \n",
    "\tdef __init__(self, gamma, epsilon, lr, input_dims, batch_size, num_actions,\n",
    "\t\t\t\t max_mem_size=10000, eps_end=0.01, eps_dec=0.99, replace_target=1000):\n",
    "\t\tself.gamma = gamma # Determines the weighting of future rewards\n",
    "\t\tself.epsilon = epsilon\n",
    "\t\tself.epsilon_end = eps_end\n",
    "\t\tself.epsilon_decay = eps_dec\n",
    "\t\tself.lr = lr\n",
    "\t\tself.action_space = [i for i in range(num_actions)]\n",
    "\t\tself.num_actions = num_actions\n",
    "\t\tself.batch_size = batch_size\n",
    "\t\tself.replace_target = replace_target # Number of steps before updating target network\n",
    "\t\tself.learn_step_counter = 0 # Track the steps for target network update\n",
    "\t\t\n",
    "\t\t# Initialize evaluation (q_eval) and target (q_target) network\n",
    "\t\tself.q_eval = DQN(self.lr, input_dims, fc1_dims=512, fc2_dims=512, num_actions=num_actions)\n",
    "\t\tself.q_target = DQN(self.lr, input_dims, fc1_dims=512, fc2_dims=512, num_actions=num_actions)\n",
    "  \n",
    "\t\t# Initially, the target network has the same weights as the evaluation network\n",
    "\t\tself.q_target.load_state_dict(self.q_eval.state_dict())\n",
    "\t\tself.q_target.eval()\n",
    "  \n",
    "\t\t# Initialize the replay buffer\n",
    "\t\tself.memory = ReplayBuffer(max_mem_size, input_dims)\n",
    "\t\tself.optimizer = Adam(self.q_eval.parameters(), lr=self.lr)\n",
    "\t\tself.loss = nn.MSELoss()\n",
    "\t\tself.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\tdef store_transition(self, state, action, reward, state_, done):\n",
    "\t\tself.memory.store_transition(state, action, reward, state_, done)\n",
    "  \n",
    "\t# Method for choosing an action\n",
    "\tdef choose_action(self, observation):\n",
    "\t\tif np.random.random() > self.epsilon:\n",
    "\t\t\tobservation = observation / 255.0\n",
    "\t\t\tstate = torch.tensor([observation], dtype=torch.float32).to(self.q_eval.device)\n",
    "\t\t\tactions = self.q_eval.forward(state)\n",
    "\t\t\taction = torch.argmax(actions).item()\n",
    "\t\telse:\n",
    "\t\t\taction = np.random.choice(self.action_space)\n",
    "\t\treturn action\n",
    "\n",
    "\tdef replace_target_network(self):\n",
    "\t\tif self.learn_step_counter % self.replace_target == 0:\n",
    "\t\t\tself.q_target.load_state_dict(self.q_eval.state_dict())\n",
    "\t\t\tprint('Target network updated.')\n",
    "\t\n",
    "\tdef learn(self):\n",
    "\t\tif self.memory.mem_cntr < self.batch_size:\n",
    "\t\t\treturn\n",
    "\t\tself.optimizer.zero_grad()\n",
    "\t\t\n",
    "\t\tself.replace_target_network()\n",
    "  \n",
    "\t\tstates, actions, rewards, states_, dones = self.memory.sample_buffer(self.batch_size)\n",
    "\t\t\n",
    "\t\tstates = torch.tensor(states).to(self.q_eval.device)\n",
    "\t\trewards = torch.tensor(rewards).to(self.q_eval.device)\n",
    "\t\tdones = torch.tensor(dones, dtype=torch.float32).to(self.q_eval.device)\n",
    "\t\tactions = torch.tensor(actions).to(self.q_eval.device)\n",
    "\t\tstates_ = torch.tensor(states_).to(self.q_eval.device)\n",
    "\t\t\n",
    "\t\t# Q-values for the next state from the target network (for stability)\n",
    "\t\tq_next = self.q_target.forward(states_).max(dim=1)[0]\n",
    "\n",
    "\t\t# Q-values for current state from evaluation network\n",
    "\t\tq_pred = self.q_eval.forward(states)[range(self.batch_size), actions]\n",
    "\t\t\n",
    "\t\t# Calculate target Q-values using Bellman equation\n",
    "\t\tq_target = rewards + self.gamma *  q_next * (1 - dones)\n",
    "\t\t\n",
    "\t\t# Compute loss between predicted Q-values and target Q-values\n",
    "\t\tloss = self.loss(q_pred, q_target)\n",
    "\t\tloss.backward()\n",
    "\t\tself.optimizer.step()\n",
    "\n",
    "\t\t# Increment the step counter\t\t\n",
    "\t\tself.learn_step_counter += 1\n",
    "  \n",
    "\t\treturn loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Environment Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Dependencies\n",
    "import torch # PyToch library for building and training neural networks\n",
    "from torch import nn\n",
    "from torch.optim import AdamW\n",
    "import numpy as np # for numerical calculations\n",
    "from collections import namedtuple, deque # provides useful data structures may not need\n",
    "import random # for random sampling \n",
    "from mss import mss # for grabbing a screen shot of a monitor \n",
    "import pydirectinput # for mouse and keyboard input on windows\n",
    "import cv2 as cv # for image and video processing\n",
    "import pytesseract # OCR tool for reading text from images\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import time\n",
    "from gymnasium import Env\n",
    "from gymnasium.spaces import Box, Discrete\n",
    "from gymnasium.utils.env_checker import check_env  # Import the environment checker\n",
    "from collections import deque\n",
    "import math\n",
    "\n",
    "\n",
    "class PacMan(Env):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Define spaces\n",
    "        self.observation_space = Box(low=0, high=255, shape=(6,50,80), dtype=np.uint8)\n",
    "        self.action_space = Discrete(4) # number of possible actions\n",
    "        \n",
    "        # Define capture locations\n",
    "        self.cap = mss()\n",
    "        self.game_location = {'top':50, 'left':-2280, 'width':1400, 'height':1300}# defines game viewing location\n",
    "        self.lives_location = {'top':1070, 'left':-902, 'width':600, 'height':200} # defines lives location\n",
    "        self.frame_stack = deque(maxlen=6) # stack frames to provide a sense of motion\n",
    "        #self.score_location = {'top':380, 'left':-920, 'width':600, 'height':80} # defines score location\n",
    "        #self.done_location = {'top':508, 'left':-1810, 'width':450, 'height':80} \n",
    "            \n",
    "        # Define lives\n",
    "        self.previous_lives = 2\n",
    "        self.current_lives = self.previous_lives\n",
    "        self.previous_score = 0\n",
    "        self.time_alive = 0\n",
    "        self.last_life = 2\n",
    "        self.survival_reward_factor = 0.01\n",
    "        \n",
    "        # Define pellet count\n",
    "        self.pellet_address = 0x7268 # ROM memory address\n",
    "        self.file_path = \"pellet_count.txt\" # file to store value\n",
    "        self.previous_pellet_count = self.read_pellet_count_from_file()\n",
    "        \n",
    "\n",
    "        # Define templates for tracking\n",
    "        self.ghost_template = cv.imread('C:\\\\Users\\\\John Wesley\\\\Docs\\\\PacMan\\\\PacManGame\\\\Images\\\\ghost_template.png', 0)\n",
    "        self.ghost_template2 = cv.imread('C:\\\\Users\\\\John Wesley\\\\Docs\\\\PacMan\\\\PacManGame\\\\Images\\\\ghost_template3.png', 0)\n",
    "        self.ghost_template3 = cv.imread('C:\\\\Users\\\\John Wesley\\\\Docs\\\\PacMan\\\\PacManGame\\\\Images\\\\ghost_template4.png', 0)\n",
    "        self.pacman_life_template = cv.imread('C:\\\\Users\\\\John Wesley\\\\Docs\\\\PacMan\\\\PacManGame\\\\Images\\\\pacman_life_icon.png', 0)\n",
    "        self.pacman_template_left = cv.imread('C:\\\\Users\\\\John Wesley\\\\Docs\\\\PacMan\\\\PacManGame\\\\Images\\\\pacman_template_left.png', 0)\n",
    "        self.pacman_template_right = cv.imread('C:\\\\Users\\\\John Wesley\\\\Docs\\\\PacMan\\\\PacManGame\\\\Images\\\\pacman_template_right.png', 0)\n",
    "        self.pacman_template_up = cv.imread('C:\\\\Users\\\\John Wesley\\\\Docs\\\\PacMan\\\\PacManGame\\\\Images\\\\pacman_template_up.png', 0)\n",
    "        self.pacman_template_down = cv.imread('C:\\\\Users\\\\John Wesley\\\\Docs\\\\PacMan\\\\PacManGame\\\\Images\\\\pacman_template_down.png', 0)\n",
    "        self.pacman_template_closed = cv.imread('C:\\\\Users\\\\John Wesley\\\\Docs\\\\PacMan\\\\PacManGame\\\\Images\\\\pacman_template_closed.png', 0)\n",
    "        \n",
    "    # Observation of the state of the environment\n",
    "    def get_observation(self):\n",
    "        # Get screen capture of game\n",
    "        raw = np.array(self.cap.grab(self.game_location))[:,:,:3]\n",
    "        # Grayscale\n",
    "        gray = cv.cvtColor(raw, cv.COLOR_BGR2GRAY)\n",
    "        # Resize\n",
    "        resized = cv.resize(gray, (80,50))\n",
    "        # Add channels first\n",
    "        channel = np.reshape(resized, (1,50,80))\n",
    "        return channel\n",
    "    \n",
    "    def get_stacked_observation(self):\n",
    "        # Stack the frames in the deque and convert to the required shape\n",
    "        return np.concatenate(list(self.frame_stack), axis=0)\n",
    "    \n",
    "    # Get number of lives left\n",
    "    def get_lives(self):   \n",
    "        # Capture the area where the lives are displayed\n",
    "        lives_cap = np.array(self.cap.grab(self.lives_location))[:,:,:3]\n",
    "        # Convert to grayscale\n",
    "        lives_gray = cv.cvtColor(lives_cap, cv.COLOR_BGR2GRAY)\n",
    "        # Perform template matching\n",
    "        result = cv.matchTemplate(lives_gray, self.pacman_life_template, cv.TM_CCORR_NORMED)\n",
    "        locations = np.where(result >= 0.8) # find areas that have values at or above threshold value\n",
    "        lives_value = len(list(zip(*locations[::-1])))\n",
    "        \n",
    "        # Determine number of lives\n",
    "        if lives_value == 684:\n",
    "            num_lives = 2 \n",
    "        elif lives_value == 344:\n",
    "            num_lives = 1\n",
    "        else:\n",
    "            num_lives = 0\n",
    "            \n",
    "        return num_lives\n",
    "    \n",
    "    # Get game over\n",
    "    def get_done(self):\n",
    "        # Get the number of lives left \n",
    "        num_lives = self.get_lives()\n",
    "        return num_lives == 0 # return bool\n",
    "    \n",
    "    # Get pellet count\n",
    "    def read_pellet_count_from_file(self):\n",
    "        try:\n",
    "            with open(self.file_path, \"r\") as file:\n",
    "                return int(file.read().strip())\n",
    "        except (FileNotFoundError, ValueError):\n",
    "            return 0\n",
    "        \n",
    "    # Resets the environment to its initial state\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        # restart the game\n",
    "        pydirectinput.click(x=-890, y=374) # Select game window\n",
    "        pydirectinput.press('f1') # Start state 1 save\n",
    "        # Reset pellet count\n",
    "        self.previous_pellet_count = self.read_pellet_count_from_file()\n",
    "        # Reset frame stack\n",
    "        self.frame_stack.clear() # Delete all items from Deque\n",
    "        # Update deque with reset state\n",
    "        for _ in range(6):\n",
    "            initial_frame = self.get_observation()\n",
    "            self.frame_stack.append(initial_frame)\n",
    "            \n",
    "        return self.get_stacked_observation(), {}\n",
    "    \n",
    "    # Rendering method to see what the computer sees\n",
    "    def render(self):\n",
    "        frame = self.render_positions()\n",
    "        cv.imshow('Game', frame)\n",
    "        if cv.waitKey(1) & 0xFF == ord('q'):\n",
    "            self.close()\n",
    "            \n",
    "    # Closes rendering window        \n",
    "    def close(self):\n",
    "        cv.destroyAllWindows()\n",
    "    \n",
    "    # Find character locations on screen            \n",
    "    def get_character_positions(self):\n",
    "        screen_capture = np.array(self.cap.grab(self.game_location))[:,:,:3]\n",
    "        cv.imwrite('game_capture.png', screen_capture)\n",
    "        # Convert to grayscale\n",
    "        gray_screen = cv.cvtColor(screen_capture, cv.COLOR_BGR2GRAY)\n",
    "        \n",
    "        # Match the templates to find Pac-Man and Ghosts\n",
    "        result_left = cv.matchTemplate(gray_screen, self.pacman_template_left, cv.TM_CCOEFF_NORMED)\n",
    "        result_right = cv.matchTemplate(gray_screen, self.pacman_template_right, cv.TM_CCOEFF_NORMED)\n",
    "        result_up = cv.matchTemplate(gray_screen, self.pacman_template_up, cv.TM_CCOEFF_NORMED)\n",
    "        result_down = cv.matchTemplate(gray_screen, self.pacman_template_down, cv.TM_CCOEFF_NORMED)\n",
    "        result_closed = cv.matchTemplate(gray_screen, self.pacman_template_closed, cv.TM_CCOEFF_NORMED)\n",
    "        result_ghost = cv.matchTemplate(gray_screen, self.ghost_template, cv.TM_CCOEFF_NORMED)\n",
    "        result_ghost2 = cv.matchTemplate(gray_screen, self.ghost_template2, cv.TM_CCOEFF_NORMED)\n",
    "        result_ghost3 = cv.matchTemplate(gray_screen, self.ghost_template3, cv.TM_CCOEFF_NORMED)\n",
    "        \n",
    "        # Locate pacman\n",
    "        pacman_threshold = 0.6 # Adjust this value based on testing\n",
    "        locations_left = np.where(result_left >= pacman_threshold)\n",
    "        locations_right = np.where(result_right >= pacman_threshold)\n",
    "        locations_up = np.where(result_up >= pacman_threshold)\n",
    "        locations_down = np.where(result_down >= pacman_threshold)\n",
    "        locations_closed = np.where(result_closed >= pacman_threshold)\n",
    "        \n",
    "        # Locate ghosts\n",
    "        ghost_threshold = 0.5\n",
    "        location_ghost = np.where(result_ghost >= ghost_threshold)\n",
    "        location_ghost2 = np.where(result_ghost2 >= ghost_threshold)\n",
    "        location_ghost3 = np.where(result_ghost3 >= ghost_threshold)\n",
    "        \n",
    "        # Pack locations\n",
    "        pacman_combined_locations = list(zip(*locations_left[::-1])) + list(zip(*locations_right[::-1])) + list(zip(*locations_up[::-1])) + list(zip(*locations_down[::-1])) + list(zip(*locations_closed[::-1]))\n",
    "        ghost_position = list(zip(*location_ghost[::-1])) + list(zip(*location_ghost2[::-1]))  + list(zip(*location_ghost3[::-1]))\n",
    "\n",
    "        return ghost_position, pacman_combined_locations, screen_capture\n",
    "    \n",
    "    # Method to see character detection    \n",
    "    def render_positions(self):\n",
    "        # Get character positions\n",
    "        ghost_position, pacman_combined_locations, screen_capture = self.get_character_positions()\n",
    "\n",
    "        screen_capture = np.ascontiguousarray(screen_capture) # convert captured image to OpenCV compatability\n",
    "        \n",
    "        # Draw rectangles around matched Pac-Man locations using OpenCV\n",
    "        for loc in pacman_combined_locations:\n",
    "            top_left = loc\n",
    "            bottom_right = (top_left[0] + self.pacman_template_right.shape[1], top_left[1] + self.pacman_template_right.shape[0])\n",
    "            # Create a rectangle patch and add it to the plot\n",
    "            cv.rectangle(screen_capture, top_left, bottom_right, (255, 0, 0), 2)\n",
    "            \n",
    "        # Draw rectangles around matched Ghost locations using OpenCV\n",
    "        for loc in ghost_position:\n",
    "            top_left = loc\n",
    "            bottom_right = (top_left[0] + self.ghost_template.shape[1], top_left[1] + self.ghost_template.shape[0])\n",
    "            # Create a rectangle patch and add it to the plot\n",
    "            cv.rectangle(screen_capture, top_left, bottom_right, (0, 0, 255), 2)\n",
    "\n",
    "        # cv.imshow('Test Render positions', screen_capture)\n",
    "        # cv.waitKey(0)\n",
    "        # cv.destroyAllWindows()\n",
    "        return screen_capture\n",
    "    \n",
    "    def calculate_distance (self, pacman_pos, ghost_pos):\n",
    "        # Unpack positions\n",
    "        pacman_x, pacman_y = pacman_pos\n",
    "        ghost_x, ghost_y = ghost_pos\n",
    "        return math.sqrt((ghost_x - pacman_x) ** 2 + (ghost_y - pacman_y) ** 2)\n",
    "    \n",
    "    # Calculate reward for eating pellets\n",
    "    def get_pellet_reward(self, current_pellet_count):\n",
    "        if current_pellet_count < self.previous_pellet_count:\n",
    "            reward = 20\n",
    "            self.previous_pellet_count = current_pellet_count\n",
    "        else:\n",
    "            reward = 0    \n",
    "        return reward\n",
    "    \n",
    "    def ghost_avoidance_reward(self):\n",
    "        ghost_positions, pacman_combined_locations, _ = self.get_character_positions()\n",
    "        if pacman_combined_locations:\n",
    "            pacman_pos = pacman_combined_locations[0]\n",
    "        else:\n",
    "            pacman_pos = (0, 0)\n",
    "        safe_distance = 260\n",
    "        avoidance_reward = 0\n",
    "        num_considered_ghosts = min(len(ghost_positions), 4)\n",
    "        \n",
    "        for ghost_pos in ghost_positions[:num_considered_ghosts]:\n",
    "            distance = self.calculate_distance(pacman_pos, ghost_pos)\n",
    "            #print(f\"Ghost {ghost_index + 1} Position: {ghost_pos}, Distance: {distance}\")\n",
    "            if distance > safe_distance:\n",
    "                # avoidance_reward += (distance - safe_distance)\n",
    "                avoidance_reward += 20 / num_considered_ghosts\n",
    "            else:\n",
    "                # avoidance_reward -= (safe_distance - distance) * 2\n",
    "                avoidance_reward -= 10\n",
    "        return avoidance_reward\n",
    "    \n",
    "    # Method that is called to do something in the game\n",
    "    def step(self, action):\n",
    "        action_map = {\n",
    "            0: 'left',   # Move Left\n",
    "            1: 'right',  # Move Right\n",
    "            2: 'up',     # Move Up\n",
    "            3: 'down',   # Move Down\n",
    "        }\n",
    "        \n",
    "        pydirectinput.press(action_map[action])\n",
    "        \n",
    "        # Reward for eating pellets \n",
    "        # current_pellet_count = self.read_pellet_count_from_file()\n",
    "        # pellet_reward = self.get_pellet_reward(current_pellet_count)\n",
    "        # Reward for avoiding ghosts\n",
    "        avoidance_reward = self.ghost_avoidance_reward()\n",
    "        # Bonus reward for staying alive      \n",
    "        current_lives = self.get_lives()\n",
    "        # if current_lives < self.last_life:\n",
    "        #     self.time_alive = 0\n",
    "        #     self.last_life = current_lives\n",
    "        # self.time_alive += 1\n",
    "        # # survival_reward = self.survival_reward_factor * (1.1 ** self.time_alive)\n",
    "        # survival_reward = self.time_alive * 1.01 \n",
    "        # Penalize only when a life is lost (and only once per life loss)\n",
    "        life_penalty = 0\n",
    "        if current_lives < self.previous_lives:\n",
    "            life_penalty -= 40\n",
    "            self.previous_lives = current_lives # update previous lives \n",
    "           \n",
    "        reward = avoidance_reward + life_penalty \n",
    "        \n",
    "        done = self.get_done()\n",
    "        \n",
    "        # Get the next observation\n",
    "        new_frame = self.get_observation()\n",
    "        self.frame_stack.append(new_frame)\n",
    "        stacked_observation = self.get_stacked_observation()\n",
    "        \n",
    "        return stacked_observation, reward, done, False, {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = PacMan()\n",
    "\n",
    "# check_env(env, warn=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "| episodes   |   0    |\n",
      "| score      |  160.0 |\n",
      "| epsilon    |  1.00   |\n",
      "------------------------\n",
      "\n",
      "------------------------\n",
      "| episodes   |   1    |\n",
      "| score      |  280.0 |\n",
      "| epsilon    |  1.00   |\n",
      "------------------------\n",
      "\n",
      "------------------------\n",
      "| episodes   |   2    |\n",
      "| score      |  225.0 |\n",
      "| epsilon    |  1.00   |\n",
      "------------------------\n",
      "\n",
      "------------------------\n",
      "| episodes   |   3    |\n",
      "| score      |  175.0 |\n",
      "| epsilon    |  1.00   |\n",
      "------------------------\n",
      "\n",
      "------------------------\n",
      "| episodes   |   4    |\n",
      "| score      |  160.0 |\n",
      "| epsilon    |  1.00   |\n",
      "------------------------\n",
      "\n",
      "------------------------\n",
      "| episodes   |   5    |\n",
      "| score      |  240.0 |\n",
      "| epsilon    |  1.00   |\n",
      "------------------------\n",
      "\n",
      "------------------------\n",
      "| episodes   |   6    |\n",
      "| score      |  195.0 |\n",
      "| epsilon    |  1.00   |\n",
      "------------------------\n",
      "\n",
      "------------------------\n",
      "| episodes   |   7    |\n",
      "| score      |  225.0 |\n",
      "| epsilon    |  1.00   |\n",
      "------------------------\n",
      "\n",
      "------------------------\n",
      "| episodes   |   8    |\n",
      "| score      |  240.0 |\n",
      "| epsilon    |  1.00   |\n",
      "------------------------\n",
      "\n",
      "------------------------\n",
      "| episodes   |   9    |\n",
      "| score      |  -40.0 |\n",
      "| epsilon    |  1.00   |\n",
      "------------------------\n",
      "\n",
      "------------------------\n",
      "| episodes   |   10    |\n",
      "| score      |  260.0 |\n",
      "| epsilon    |  1.00   |\n",
      "------------------------\n",
      "\n",
      "------------------------\n",
      "| episodes   |   11    |\n",
      "| score      |  140.0 |\n",
      "| epsilon    |  1.00   |\n",
      "------------------------\n",
      "\n",
      "------------------------\n",
      "| episodes   |   12    |\n",
      "| score      |  220.0 |\n",
      "| epsilon    |  1.00   |\n",
      "------------------------\n",
      "\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    env = PacMan()\n",
    "    \n",
    "    agent = DQNAgent(gamma=0.99, epsilon=1.0, batch_size=64, num_actions=4, \n",
    "                     eps_end=0.01, eps_dec=0.998, input_dims=(6, 50, 80), lr=0.001)\n",
    "    agent.q_eval.to(device)\n",
    "    agent.q_target.to(device)\n",
    "    \n",
    "    scores, eps_history = [], []\n",
    "    n_games = 220\n",
    "    best_score = -np.inf # initialize best score to a low value\n",
    "    save_folder = 'saved_models'\n",
    "    learn_interval = 10 # Call learn() every 10 steps\n",
    "    warmup_eps = 20 # Episodes before the agent starts learning\n",
    "    step_counter = 0 \n",
    "    \n",
    "    for i in range(n_games):\n",
    "        score = 0\n",
    "        done = False\n",
    "        observation, _ = env.reset()\n",
    "        loss = None\n",
    "        previous_loss = float('inf')\n",
    "        has_learned = False\n",
    "        while not done:\n",
    "            action = agent.choose_action(observation)\n",
    "            observation_, reward, done, truncated, info = env.step(action)\n",
    "            score += reward\n",
    "            \n",
    "            agent.store_transition(observation, action, reward, observation_, done)\n",
    "            \n",
    "            step_counter += 1\n",
    "            # Start learning only after warm-up period and learn periodically\n",
    "            if i > warmup_eps and step_counter % learn_interval == 0:\n",
    "                loss = agent.learn() # this is batch learning\n",
    "                has_learned = True\n",
    "            observation = observation_\n",
    "            # env.render()\n",
    "            \n",
    "        scores.append(score)\n",
    "        eps_history.append(agent.epsilon)\n",
    "        \n",
    "        # Calcualte average score over the last 100 games\n",
    "        avg_score = np.mean(scores[-100:])\n",
    "        \n",
    "        print('------------------------')\n",
    "        print(f'| episodes   |   {i}    |')\n",
    "        print(f'| score      |  {score} |')\n",
    "        print(f'| epsilon    |  {agent.epsilon:.2f}   |')\n",
    "        print('------------------------')\n",
    "        if loss is not None:\n",
    "            print(f', loss {loss:.4f}')\n",
    "        else: \n",
    "            print() # Just print a new line if loss is None\n",
    "            \n",
    "        if has_learned and loss < previous_loss:\n",
    "            agent.epsilon = max(agent.epsilon_end, agent.epsilon * agent.epsilon_decay)\n",
    "            # print(f\"Updated epsilon: {agent.epsilon}\") # debug statement\n",
    "        previous_loss = loss if loss is not None else previous_loss\n",
    "        # Save the model if the average score improves\n",
    "        # Save the model every 10 episodes\n",
    "        if has_learned and i % 10 == 0:\n",
    "            model_filename = f'{save_folder}/best_pacman_dqn_model_{i}.pth'\n",
    "            torch.save(agent.q_eval.state_dict(), model_filename)\n",
    "            print(f'Model saved as {model_filename}')\n",
    "\n",
    "    x = [i+1 for i in range(n_games)]\n",
    "    filename = 'pacman_plot.png'\n",
    "    plot_learning_curve(x, scores, eps_history, filename)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # To load the model later\n",
    "# model_filename = 'pacman_dqn_model.pth'\n",
    "# agent.q_eval.load_state_dict(torch.load(model_filename))\n",
    "# agent.q_eval.eval()  # Set the model to evaluation mode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "| episodes   |   219    |\n",
      "| score      |  47412.24999999999 |\n",
      "| epsilon    |  0.68   |\n",
      "------------------------\n"
     ]
    }
   ],
   "source": [
    "print('------------------------')\n",
    "print(f'| episodes   |   {i}    |')\n",
    "print(f'| score      |  {score} |')\n",
    "print(f'| epsilon    |  {agent.epsilon:.2f}   |')\n",
    "print('------------------------')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuralnet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
