{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary Results\n",
    "\n",
    "**Resources & References**\n",
    "1. The Game: https://dinosaurgame.app/ \n",
    "2. Video Tutorial: https://www.youtube.com/watch?v=vahwuupy81A&t=5517s\n",
    "3. Stable-Baselines3 Documentations: https://stable-baselines3.readthedocs.io/en/master/\n",
    "4. Gymnasium Documentation: https://gymnasium.farama.org/tutorials/gymnasium_basics/environment_creation/\n",
    "5. PyTorch DQN Documentation for Gym Retro: https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Dependencies\n",
    "import torch # PyToch library for building and training neural networks\n",
    "from torch import nn\n",
    "from torch.optim import AdamW\n",
    "import numpy as np # for numerical calculations\n",
    "from collections import namedtuple, deque # provides useful data structures may not need\n",
    "import random # for random sampling \n",
    "from mss import mss # for grabbing a screen shot of a monitor \n",
    "import pydirectinput # for mouse and keyboard input on windows\n",
    "import cv2 as cv # for image and video processing\n",
    "import pytesseract # OCR tool for reading text from images\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import time\n",
    "from gymnasium import Env\n",
    "from gymnasium.spaces import Box, Discrete\n",
    "from gymnasium.utils import env_checker  # Import the environment checker\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PacMan(Env):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Define spaces\n",
    "        self.observation_space = Box(low=0, high=255, shape=(6,50,80), dtype=np.uint8)\n",
    "        self.action_space = Discrete(5) # number of possible actions\n",
    "        \n",
    "        self.previous_lives = 2\n",
    "        self.current_lives = self.previous_lives\n",
    "        self.previous_score = 0\n",
    "        \n",
    "        self.pellet_address = 0x7268\n",
    "        self.file_path = \"pellet_count.txt\"\n",
    "        self.previous_pellet_count = self.read_pellet_count_from_file()\n",
    "        \n",
    "        # Define capture locations\n",
    "        self.cap = mss()\n",
    "        self.game_location = {'top':50, 'left':-2280, 'width':1400, 'height':1300}# defines game viewing location\n",
    "        self.lives_location = {'top':1070, 'left':-902, 'width':600, 'height':200} # defines lives location\n",
    "        self.frame_stack = deque(maxlen=6) # stack frames to provide a sense of motion\n",
    "        #self.score_location = {'top':380, 'left':-920, 'width':600, 'height':80} # defines score location\n",
    "        #self.done_location = {'top':508, 'left':-1810, 'width':450, 'height':80}     \n",
    "\n",
    "        # Define templates for tracking\n",
    "        self.ghost_template = cv.imread('ghost_template.png', 0)\n",
    "        self.ghost_template2 = cv.imread('ghost_template3.png', 0)\n",
    "        self.ghost_template3 = cv.imread('ghost_template4.png', 0)\n",
    "        self.pacman_life_template = cv.imread('pacman_life_icon.png', 0)\n",
    "        self.pacman_template_left = cv.imread('pacman_template_left.png', 0)\n",
    "        self.pacman_template_right = cv.imread('pacman_template_right.png', 0)\n",
    "        self.pacman_template_up = cv.imread('pacman_template_up.png', 0)\n",
    "        self.pacman_template_down = cv.imread('pacman_template_down.png', 0)\n",
    "        self.pacman_template_closed = cv.imread('pacman_template_closed.png', 0)\n",
    "        \n",
    "    # observation of the state of the environment\n",
    "    def get_observation(self):\n",
    "        # Get screen capture of game\n",
    "        raw = np.array(self.cap.grab(self.game_location))[:,:,:3]\n",
    "        # Grayscale\n",
    "        gray = cv.cvtColor(raw, cv.COLOR_BGR2GRAY)\n",
    "        # Resize\n",
    "        resized = cv.resize(gray, (80,50))\n",
    "        # Add channels first\n",
    "        channel = np.reshape(resized, (1,50,80))\n",
    "        return channel\n",
    "    \n",
    "    def get_stacked_observation(self):\n",
    "        # stack the frames in the deque and convert to the required shape\n",
    "        return np.concatenate(list(self.frame_stack), axis=0)\n",
    "    \n",
    "    # get number of lives left\n",
    "    def get_lives(self):   \n",
    "        # Capture the area where the lives are displayed\n",
    "        lives_cap = np.array(self.cap.grab(self.lives_location))[:,:,:3]\n",
    "        # Convert to grayscale\n",
    "        lives_gray = cv.cvtColor(lives_cap, cv.COLOR_BGR2GRAY)\n",
    "        \n",
    "        # Perform template matching\n",
    "        result = cv.matchTemplate(lives_gray, self.pacman_life_template, cv.TM_CCORR_NORMED)\n",
    "        threshold = 0.8\n",
    "        locations = np.where(result >= threshold)\n",
    "        \n",
    "        lives_value = len(list(zip(*locations[::-1])))\n",
    "        \n",
    "        # Determine number of lives\n",
    "        if lives_value == 684:\n",
    "            num_lives = 2\n",
    "        elif lives_value == 344:\n",
    "            num_lives = 1\n",
    "        else:\n",
    "            num_lives = 0\n",
    "            \n",
    "        return num_lives\n",
    "    \n",
    "    # Get game over\n",
    "    def get_done(self):\n",
    "        # Get the number of lives left \n",
    "        num_lives = self.get_lives()\n",
    "        return num_lives == 0 # return bool\n",
    "    \n",
    "    def read_pellet_count_from_file(self):\n",
    "        try:\n",
    "            with open(self.file_path, \"r\") as file:\n",
    "                return int(file.read().strip())\n",
    "        except (FileNotFoundError, ValueError):\n",
    "            return 0\n",
    "        \n",
    "    # Resets the environment to its initial state\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        # restart the game\n",
    "        pydirectinput.click(x=-890, y=374) # select game window\n",
    "        pydirectinput.press('f1') # Start state 1 save\n",
    "        \n",
    "        # reset pellet count\n",
    "        self.previous_pellet_count = self.read_pellet_count_from_file()\n",
    "        \n",
    "        # reset frame stack\n",
    "        self.frame_stack.clear()\n",
    "        for _ in range(6):\n",
    "            initial_frame = self.get_observation()\n",
    "            self.frame_stack.append(initial_frame)\n",
    "            \n",
    "        return self.get_stacked_observation(), {}\n",
    "    def render(self):\n",
    "        frame = self.render_positions()\n",
    "        \n",
    "        cv.imshow('Game', frame)\n",
    "        \n",
    "        if cv.waitKey(1) & 0xFF == ord('q'):\n",
    "            self.close()\n",
    "            \n",
    "    def close(self):\n",
    "        cv.destroyAllWindows()\n",
    "               \n",
    "    def get_character_positions(self):\n",
    "        # Capture the area where the lives are displayed\n",
    "        screen_capture = np.array(self.cap.grab(self.game_location))[:,:,:3]\n",
    "        cv.imwrite('game_capture.png', screen_capture)\n",
    "        # Convert to grayscale\n",
    "        gray_screen = cv.cvtColor(screen_capture, cv.COLOR_BGR2GRAY)\n",
    "        # Match the templates to find Pac-Man\n",
    "        result_left = cv.matchTemplate(gray_screen, self.pacman_template_left, cv.TM_CCOEFF_NORMED)\n",
    "        result_right = cv.matchTemplate(gray_screen, self.pacman_template_right, cv.TM_CCOEFF_NORMED)\n",
    "        result_up = cv.matchTemplate(gray_screen, self.pacman_template_up, cv.TM_CCOEFF_NORMED)\n",
    "        result_down = cv.matchTemplate(gray_screen, self.pacman_template_down, cv.TM_CCOEFF_NORMED)\n",
    "        result_closed = cv.matchTemplate(gray_screen, self.pacman_template_closed, cv.TM_CCOEFF_NORMED)\n",
    "        result_ghost = cv.matchTemplate(gray_screen, self.ghost_template, cv.TM_CCOEFF_NORMED)\n",
    "        result_ghost2 = cv.matchTemplate(gray_screen, self.ghost_template2, cv.TM_CCOEFF_NORMED)\n",
    "        result_ghost3 = cv.matchTemplate(gray_screen, self.ghost_template3, cv.TM_CCOEFF_NORMED)\n",
    "        threshold = 0.6 # Adjust this value based on testing\n",
    "        locations_left = np.where(result_left >= threshold)\n",
    "        locations_right = np.where(result_right >= threshold)\n",
    "        locations_up = np.where(result_up >= threshold)\n",
    "        locations_down = np.where(result_down >= threshold)\n",
    "        locations_closed = np.where(result_closed >= threshold)\n",
    "        location_ghost = np.where(result_ghost >= 0.5)\n",
    "        location_ghost2 = np.where(result_ghost2 >= 0.5)\n",
    "        location_ghost3 = np.where(result_ghost3 >= 0.5)\n",
    "        pacman_combined_locations = list(zip(*locations_left[::-1])) + list(zip(*locations_right[::-1])) + list(zip(*locations_up[::-1])) + list(zip(*locations_down[::-1])) + list(zip(*locations_closed[::-1]))\n",
    "        ghost_position = list(zip(*location_ghost[::-1])) + list(zip(*location_ghost2[::-1]))  + list(zip(*location_ghost3[::-1]))\n",
    "\n",
    "        return ghost_position, pacman_combined_locations, screen_capture\n",
    "        \n",
    "    def render_positions(self):\n",
    "        ghost_position, pacman_combined_locations, screen_capture = self.get_character_positions()\n",
    "\n",
    "        screen_capture = np.ascontiguousarray(screen_capture) # convert captured image to OpenCV compatability\n",
    "        \n",
    "        # Draw rectangles around matched locations using Matplotlib patches\n",
    "        for loc in pacman_combined_locations:\n",
    "            top_left = loc\n",
    "            bottom_right = (top_left[0] + self.pacman_template_right.shape[1], top_left[1] + self.pacman_template_right.shape[0])\n",
    "            # Create a rectangle patch and add it to the plot\n",
    "            cv.rectangle(screen_capture, top_left, bottom_right, (255, 0, 0), 2)\n",
    "\n",
    "        for loc in ghost_position:\n",
    "            top_left = loc\n",
    "            bottom_right = (top_left[0] + self.ghost_template.shape[1], top_left[1] + self.ghost_template.shape[0])\n",
    "            # Create a rectangle patch and add it to the plot\n",
    "            cv.rectangle(screen_capture, top_left, bottom_right, (0, 0, 255), 2)\n",
    "\n",
    "        # cv.imshow('Test Render positions', screen_capture)\n",
    "        # cv.waitKey(0)\n",
    "        # cv.destroyAllWindows()\n",
    "        return screen_capture\n",
    "    \n",
    "    def calculate_distance (self, pos1, pos2):\n",
    "        return np.sqrt((pos1[0] - pos2[0])**2 + ( pos1[1] - pos2[1])**2)\n",
    "    \n",
    "    # Reward for eating pellets\n",
    "    def get_pellet_reward(self, current_pellet_count):\n",
    "        if current_pellet_count < self.previous_pellet_count:\n",
    "            reward = 30 \n",
    "            self.previous_pellet_count = current_pellet_count\n",
    "        else:\n",
    "            reward = 0    \n",
    "        return reward\n",
    "    \n",
    "    \n",
    "    # Action that is called to do something in the game\n",
    "    def step(self, action):\n",
    "        action_map = {\n",
    "            0: 'left',   # Move Left\n",
    "            1: 'right',  # Move Right\n",
    "            2: 'up',     # Move Up\n",
    "            3: 'down',   # Move Down\n",
    "            4: 'no_op'   # No operation (do nothing)\n",
    "        }\n",
    "        \n",
    "        if action != 4:\n",
    "            pydirectinput.press(action_map[action])\n",
    "            \n",
    "        current_pellet_count = self.read_pellet_count_from_file()\n",
    "        pellet_reward = self.get_pellet_reward(current_pellet_count)\n",
    "        \n",
    "        # ghost_positions, pacman_positions, _ = self.get_character_positions()\n",
    "        \n",
    "        # if pacman_positions:\n",
    "        #     pacman_pos = pacman_positions[0]\n",
    "        # else:\n",
    "        #     pacman_pos = (0, 0) # Default position if not detected\n",
    "            \n",
    "        # if ghost_positions:\n",
    "        #     ghost_positions = ghost_positions[0]\n",
    "        # else:\n",
    "        #     ghost_positions = (0, 0)\n",
    "            \n",
    "        # ghost_penalty = 0\n",
    "        # threshold_distance = 50\n",
    "        # for ghost_pos in ghost_positions:\n",
    "        #     distance = self.calculate_distance(pacman_pos, ghost_pos)\n",
    "        #     if distance < threshold_distance:\n",
    "        #         ghost_penalty -= 10\n",
    "                \n",
    "        current_lives = self.get_lives()\n",
    "        life_penalty = 0\n",
    "        # Penalize only when a life is lost (and only once per life loss)\n",
    "        if current_lives < self.previous_lives:\n",
    "            life_penalty -= 50\n",
    "            self.previous_lives = current_lives # update previous lives \n",
    "            \n",
    "        reward = pellet_reward + life_penalty \n",
    "        \n",
    "        # Penalize heavily if all lives are lost\n",
    "        done = self.get_done()\n",
    "        # end_game_penalty = 0\n",
    "        # if done:\n",
    "        #     end_game_penalty -= 500\n",
    "        # else: \n",
    "        #     end_game_penalty -= 0\n",
    "\n",
    "        # Get the next observation\n",
    "        new_frame = self.get_observation()\n",
    "        self.frame_stack.append(new_frame)\n",
    "        stacked_observation = self.get_stacked_observation()\n",
    "        \n",
    "        return stacked_observation, reward, done, False, {}\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = PacMan()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "446.5254751971045\n",
      "447.0212522912082\n",
      "444.6684157886638\n",
      "445.16176834943946\n",
      "445.6568186396344\n",
      "446.1535610078664\n",
      "446.65198980862044\n",
      "442.8148597325975\n",
      "443.30576355378014\n",
      "443.79837764462366\n",
      "444.292696316291\n",
      "444.7887138855931\n",
      "445.28642467517466\n",
      "445.7858230136979\n",
      "446.2869032360237\n",
      "441.4532817864196\n",
      "441.9434352946087\n",
      "442.4353060052961\n",
      "442.92888819764283\n",
      "443.42417615642023\n",
      "443.9211641721985\n",
      "444.4198465415333\n",
      "444.9202175671499\n",
      "445.4222715581249\n",
      "445.92600283006595\n",
      "440.0920358288707\n",
      "440.58143401645964\n",
      "441.0725563895355\n",
      "441.5653971950248\n",
      "442.05995068542455\n",
      "442.55621111899444\n",
      "443.0541727599459\n",
      "443.5538298786293\n",
      "444.05517675171853\n",
      "444.5582076623937\n",
      "445.0629169005209\n",
      "445.56929876282993\n",
      "438.7311249501225\n",
      "439.21976276119454\n",
      "439.7101317913882\n",
      "440.20222625516107\n",
      "440.69604037249985\n",
      "441.1915683691156\n",
      "441.688804476636\n",
      "442.18774293279546\n",
      "442.68837798162264\n",
      "443.1907038736259\n",
      "443.69471486597627\n",
      "444.2004052226877\n",
      "444.7077692147957\n",
      "445.21680112053275\n",
      "437.85842460777206\n",
      "438.34803524140494\n",
      "438.8393783606936\n",
      "439.33244815287657\n",
      "439.82723881087674\n",
      "440.3237445334966\n",
      "440.82195952561165\n",
      "441.3218779983607\n",
      "441.82349416933454\n",
      "442.32680226276136\n",
      "442.83179650969055\n",
      "443.33847114817365\n",
      "443.8468204234429\n",
      "444.3568385880879\n",
      "437.47685653071983\n",
      "437.9691769976513\n",
      "438.46322536787505\n",
      "438.95899580712546\n",
      "439.4564824871741\n",
      "439.95567958602373\n",
      "440.45658128809924\n",
      "440.95918178443685\n",
      "441.4634752728701\n",
      "441.9694559582144\n",
      "442.477118052448\n",
      "442.9864557748916\n",
      "436.6062299143245\n",
      "437.09953099951963\n",
      "437.59456120934595\n",
      "438.09131468222466\n",
      "438.5897855627739\n",
      "439.089968002003\n",
      "439.59185615750437\n",
      "440.09544419364306\n",
      "440.6007262817437\n",
      "441.1076966002747\n",
      "441.61634933503086\n",
      "436.23044368773714\n",
      "436.7264590106718\n",
      "437.2241987813575\n",
      "437.72365711713593\n",
      "438.22482814190255\n",
      "438.72770598629853\n",
      "439.2322847879013\n",
      "439.7385586914115\n",
      "440.2465218488387\n",
      "436.35765147410905\n",
      "436.85810053151124\n",
      "437.36026339849394\n",
      "437.86413417862855\n",
      "438.36970698258796\n",
      "438.8769759283346\n",
      "436.4962771891646\n",
      "437.00114416326187\n",
      "437.50771421770384\n",
      "474.30791686414005\n",
      "475.14734556766706\n",
      "472.92494119045995\n",
      "473.76470953417373\n",
      "474.60509900337144\n",
      "475.4461063043844\n",
      "472.3822604628586\n",
      "473.22299183365976\n",
      "474.0643416246364\n",
      "474.90630654898655\n",
      "475.74888334078094\n",
      "471.0\n",
      "471.8410749394334\n",
      "472.682768884164\n",
      "473.52507853333384\n",
      "474.3680006071236\n",
      "475.2115318466083\n",
      "470.45934999742536\n",
      "471.3013897709193\n",
      "472.1440458165283\n",
      "472.9873148404722\n",
      "473.8311935700308\n",
      "474.6756787533989\n",
      "475.5207671595427\n",
      "469.9202059924642\n",
      "470.76321011735826\n",
      "471.60682777076073\n",
      "472.451055666087\n",
      "473.2958905378326\n",
      "474.14132914142806\n",
      "474.9873682530937\n",
      "475.83400466969573\n",
      "468.53921927625225\n",
      "469.38257317459073\n",
      "470.2265411479875\n",
      "471.07111989592397\n",
      "471.9163061391289\n",
      "472.7620966194308\n",
      "473.60848809961163\n",
      "474.45547736326114\n",
      "475.3030612146318\n",
      "476.1512364784954\n",
      "469.6913880411264\n",
      "470.53692735002215\n",
      "471.3830713973509\n",
      "472.2298169323915\n",
      "473.0771607253937\n",
      "473.92509956743163\n",
      "470.00425529988553\n",
      "470.85135658719304\n",
      "471.6990566028302\n",
      "472.547352124631\n",
      "194.4453650771856\n",
      "417.1198868430993\n",
      "416.076915966267\n",
      "416.09734437989385\n",
      "416.120174949497\n",
      "416.14540727971513\n",
      "415.04337122763445\n",
      "415.05903194605946\n",
      "415.07710127155894\n",
      "415.09757888959075\n",
      "415.12046444375636\n",
      "415.1457575358322\n",
      "415.17345772580404\n",
      "414.03019213579097\n",
      "414.04347597806685\n",
      "414.0591745149478\n",
      "414.07728747179556\n",
      "414.09781453178425\n",
      "414.12075533592855\n",
      "414.1461094831147\n",
      "414.1738765301356\n",
      "414.2040559917298\n",
      "413.0302652348857\n",
      "413.0435812356851\n",
      "413.05931777409404\n",
      "413.0774745734751\n",
      "413.09805131469693\n",
      "413.12104763616196\n",
      "413.14646313383827\n",
      "413.1742973612952\n",
      "413.2045498297423\n",
      "413.23722000807237\n",
      "412.0194170181789\n",
      "412.03033868879123\n",
      "412.0436870041816\n",
      "412.0594617285229\n",
      "412.07766258315917\n",
      "412.09828924663105\n",
      "412.1213413547034\n",
      "412.146818500398\n",
      "412.1747202340289\n",
      "412.20504606324266\n",
      "412.23779545306127\n",
      "411.0194642592976\n",
      "411.0304125000971\n",
      "411.04379328728464\n",
      "411.0596063833079\n",
      "411.077851507473\n",
      "411.0985283359696\n",
      "411.1216365018995\n",
      "411.1471755953092\n",
      "411.17514516322603\n",
      "411.2055447096987\n",
      "411.2383736958408\n",
      "411.2736315398788\n",
      "410.01951173084433\n",
      "410.03048667141815\n",
      "410.0439000887588\n",
      "410.0597517435721\n",
      "410.07804135310636\n",
      "410.0987685911773\n",
      "410.1219330881976\n",
      "410.1475344312093\n",
      "410.17557216392106\n",
      "410.2060457867485\n",
      "410.23895475685873\n",
      "410.27429848821873\n",
      "409.0110022970042\n",
      "409.0195594345092\n",
      "409.0305612053945\n",
      "409.04400741240545\n",
      "409.05989781448875\n",
      "409.0782321268146\n",
      "409.0990100208017\n",
      "409.12223112414705\n",
      "409.14789502085915\n",
      "409.1760012512953\n",
      "409.20654931220247\n",
      "409.23953865676276\n",
      "409.27496869464176\n",
      "408.0110292626904\n",
      "408.01960737199875\n",
      "408.0306361046925\n",
      "408.0441152620633\n",
      "408.0600446012817\n",
      "408.0784238354192\n",
      "408.09925263347395\n",
      "408.1225306204008\n",
      "408.1482573771448\n",
      "408.17643244067875\n",
      "408.2070553040454\n",
      "408.24012541640246\n",
      "408.2756421830722\n",
      "407.0196555450363\n",
      "407.0307113720045\n",
      "407.04422364160877\n",
      "407.060192109226\n",
      "407.07861648580854\n",
      "407.09949643791015\n",
      "407.1228315877163\n",
      "407.14862151307847\n",
      "407.17686574755203\n",
      "407.20756378043865\n",
      "407.2407150568322\n",
      "407.276318977669\n",
      "406.0197039553622\n",
      "406.03078701004927\n",
      "406.0443325549564\n",
      "406.06034034364893\n",
      "406.07881008493905\n",
      "406.09974144291203\n",
      "406.1231340369568\n",
      "406.1489874418007\n",
      "406.1773011875479\n",
      "406.20807475972214\n",
      "406.2413075993134\n",
      "406.2769991028289\n",
      "405.0308630215727\n",
      "405.0444420060594\n",
      "405.0604893099301\n",
      "405.07900463983566\n",
      "405.0999876573684\n",
      "405.123437979093\n",
      "405.14935517658176\n",
      "405.1777387764535\n",
      "405.20858826041683\n",
      "405.2419030653173\n",
      "404.04455199891015\n",
      "404.0606390135025\n",
      "404.0792001575929\n",
      "404.1002350902558\n",
      "404.1237434252039\n",
      "404.149724730823\n",
      "404.1781785302121\n",
      "404.2091043012267\n",
      "403.0607894598531\n",
      "403.07939664537554\n",
      "403.10048375064\n",
      "403.1240503864784\n",
      "403.15009611805874\n",
      "403.17862046492496\n",
      "402.1007336476769\n",
      "402.1243588742169\n",
      "402.15046935195784\n",
      "477.0450712458939\n",
      "477.87550680067295\n",
      "478.70659072128933\n",
      "479.5383196367106\n",
      "480.37069019664386\n",
      "472.3409785313995\n",
      "473.1691029642574\n",
      "473.99789029066363\n",
      "474.8273370394759\n",
      "475.6574397610112\n",
      "476.48819502690725\n",
      "477.3195994299836\n",
      "478.15164958410423\n",
      "478.9843421240406\n",
      "479.8176737053357\n",
      "480.6516410041684\n",
      "481.48624071721923\n",
      "471.7806693793208\n",
      "472.60977560774177\n",
      "473.4395420748039\n",
      "474.2699653151146\n",
      "475.10104188477635\n",
      "475.9327683612466\n",
      "476.7651413431985\n",
      "477.59815745038213\n",
      "478.43181332348706\n",
      "479.2661056240051\n",
      "480.1010310340939\n",
      "480.9365862564419\n",
      "481.7727680141334\n",
      "470.3923893942163\n",
      "471.2218161333365\n",
      "472.0519039258289\n",
      "472.88264929049785\n",
      "473.7140487678194\n",
      "474.54609891979936\n",
      "475.3787963298321\n",
      "476.21213760256046\n",
      "477.0461193637361\n",
      "477.88073826008093\n",
      "478.715990959149\n",
      "479.5518741491894\n",
      "480.38838453901025\n",
      "481.22551885784276\n",
      "482.06327385520666\n",
      "469.834013242975\n",
      "470.6644239795483\n",
      "471.49549308556493\n",
      "472.3272170857826\n",
      "473.15959252666534\n",
      "473.9926159762407\n",
      "474.82628402395756\n",
      "475.66059328054496\n",
      "476.49554037787175\n",
      "477.3311219688069\n",
      "478.1673347270807\n",
      "479.00417534714666\n",
      "479.8416405440445\n",
      "480.6797270532636\n",
      "481.5184316306075\n",
      "482.35775105205886\n",
      "469.2771036392038\n",
      "470.1084981150628\n",
      "470.9405482648526\n",
      "471.7732506194051\n",
      "472.60660173129196\n",
      "473.44059817468127\n",
      "474.2752365451943\n",
      "475.11051345976335\n",
      "475.94642555649057\n",
      "476.78296949450703\n",
      "477.6201419538334\n",
      "478.4579396352411\n",
      "479.2963592601137\n",
      "480.1353975703104\n",
      "480.9750513280289\n",
      "469.5540437478949\n",
      "470.3870746523548\n",
      "471.22075506072525\n",
      "472.05508153180597\n",
      "472.8900506460249\n",
      "473.7256590052939\n",
      "474.5619032328659\n",
      "475.39877997319263\n",
      "476.2362858917829\n",
      "477.07441767506253\n",
      "477.91317203023397\n",
      "478.7525456851378\n",
      "479.59253538811464\n",
      "480.4331379078675\n",
      "469.00106609686935\n",
      "469.83507744739535\n",
      "470.669735589617\n",
      "471.5050370886826\n",
      "472.3409785313995\n",
      "473.17755652608884\n",
      "474.01476770244193\n",
      "474.8526087113769\n",
      "475.69107622489616\n",
      "476.53016693594543\n",
      "477.36987755827244\n",
      "478.2102048262877\n",
      "479.0511454949252\n",
      "470.120197396368\n",
      "470.95647357266466\n",
      "471.79338698205595\n",
      "472.6309342393915\n",
      "473.46911198091897\n",
      "474.30791686414005\n",
      "475.14734556766706\n",
      "475.9873947910806\n",
      "476.82806125478817\n",
      "477.66934169988343\n",
      "470.40939616465994\n",
      "471.2472811592657\n",
      "472.0857972868915\n",
      "472.92494119045995\n",
      "473.76470953417373\n",
      "474.60509900337144\n",
      "475.4461063043844\n",
      "476.28772816439437\n",
      "471.5421508200513\n",
      "472.3822604628586\n",
      "473.22299183365976\n",
      "474.0643416246364\n",
      "474.90630654898655\n",
      "475.74888334078094\n",
      "471.8410749394334\n",
      "472.682768884164\n",
      "473.52507853333384\n",
      "474.3680006071236\n",
      "472.1440458165283\n",
      "472.9873148404722\n",
      "472.451055666087\n",
      "185.9139585937538\n",
      "185.27007313648906\n",
      "187.32325002519042\n",
      "186.67886864881092\n",
      "186.03763060198332\n",
      "185.39956850003725\n",
      "189.38056922503955\n",
      "188.73261509341728\n",
      "188.08774548066654\n",
      "187.4459922217597\n",
      "186.80738743422327\n",
      "186.17196351760379\n",
      "185.53975315279473\n",
      "184.91078930121952\n",
      "190.79046097748179\n",
      "190.14205216100936\n",
      "189.49670181826386\n",
      "188.85444130334875\n",
      "188.2153022471871\n",
      "187.57931655702342\n",
      "186.94651641579205\n",
      "186.31693428134759\n",
      "185.6906028855526\n",
      "192.20041623263984\n",
      "191.55155963865187\n",
      "190.90573590125572\n",
      "190.26297590440447\n",
      "189.62331080328704\n",
      "188.98677202386415\n",
      "188.35339126227592\n",
      "187.72320048411703\n",
      "187.09623192357455\n",
      "186.47251808242416\n",
      "185.85209172887994\n",
      "192.96113598338914\n",
      "192.31484602078956\n",
      "191.67159413955946\n",
      "191.03141102970474\n",
      "190.3943276465977\n",
      "189.76037521042164\n",
      "189.12958520548815\n",
      "188.50198937942272\n",
      "187.8776197422141\n",
      "187.25650856512306\n",
      "186.63868837944614\n",
      "186.02419197512995\n",
      "194.3707796969493\n",
      "193.72403051764127\n",
      "193.08029417835473\n",
      "192.43960091415696\n",
      "191.80198122021577\n",
      "191.1674658512792\n",
      "190.53608582103286\n",
      "189.90787240133042\n",
      "189.2828571212935\n",
      "188.66107176627614\n",
      "188.04254837669055\n",
      "187.42731924668826\n",
      "186.81541692269406\n",
      "195.13328778042973\n",
      "194.48907424325924\n",
      "193.84787850270635\n",
      "193.2097306038182\n",
      "192.57466084612483\n",
      "191.94269978303421\n",
      "191.31387822110554\n",
      "190.68822721919673\n",
      "190.06577808748213\n",
      "189.44656238633627\n",
      "188.83061192507955\n",
      "188.2179587605816\n",
      "187.608635195718\n",
      "187.00267377767625\n",
      "196.5426162439078\n",
      "195.8979326077741\n",
      "195.25624189766637\n",
      "194.6175737183053\n",
      "193.98195792392653\n",
      "193.34942461771124\n",
      "192.7200041511\n",
      "192.09372712298546\n",
      "191.47062437878037\n",
      "190.85072700935672\n",
      "190.23406634985227\n",
      "189.62067397834025\n",
      "189.01058171435798\n",
      "188.403821617291\n",
      "187.8004259846074\n",
      "197.3068675946177\n",
      "196.66468925559565\n",
      "196.02550854416882\n",
      "195.38935487891862\n",
      "194.7562579225633\n",
      "194.12624758130983\n",
      "193.49935400408964\n",
      "192.87560758167425\n",
      "192.25503894566717\n",
      "191.637678967368\n",
      "191.02355875650522\n",
      "190.41270965983338\n",
      "189.805163259591\n",
      "189.20095137181525\n",
      "188.6001060445089\n",
      "198.07321878537743\n",
      "197.43353311937665\n",
      "196.7968495682794\n",
      "196.16319736382766\n",
      "195.5326059765992\n",
      "194.90510511528424\n",
      "194.28072472584614\n",
      "193.65949499056327\n",
      "193.04144632694815\n",
      "192.4266093865399\n",
      "191.81501505356664\n",
      "191.2066944434739\n",
      "190.60167890131504\n",
      "190.0\n",
      "189.40168953839878\n",
      "198.84164553734712\n",
      "198.20443990990717\n",
      "197.57024067404484\n",
      "196.93907687404246\n",
      "196.31097778779463\n",
      "195.68597292601225\n",
      "195.0640920313116\n",
      "194.4453650771856\n",
      "193.82982226685345\n",
      "193.2174940319846\n",
      "192.60841103129428\n",
      "192.00260414900626\n",
      "191.4001044931794\n",
      "190.80094339389416\n",
      "190.20515240129538\n",
      "199.6121238802894\n",
      "198.97738564972653\n",
      "198.3456578803781\n",
      "197.71696942852427\n",
      "197.0913493789111\n",
      "196.468827043885\n",
      "195.8494319624134\n",
      "195.2331938989884\n",
      "194.62014284240982\n",
      "194.0103090044444\n",
      "193.40372281835735\n",
      "192.80041493731284\n",
      "192.20041623263984\n",
      "191.60375779195982\n",
      "191.01047091717248\n",
      "199.75234666956982\n",
      "199.12307751739877\n",
      "198.4968513604183\n",
      "197.87369708983556\n",
      "197.25364381932212\n",
      "196.6367208839692\n",
      "196.02295783912658\n",
      "195.41238445912276\n",
      "194.80503073586164\n",
      "194.20092687729377\n",
      "193.60010330575756\n",
      "193.00259065618783\n",
      "192.40841977418765\n",
      "200.52929960482084\n",
      "199.90247622278218\n",
      "199.2786993132984\n",
      "198.65799757371965\n",
      "198.04039991880444\n",
      "197.4259354796122\n",
      "196.8146336022807\n",
      "196.2065238466856\n",
      "195.6016359849784\n",
      "195.0\n",
      "194.40164608356588\n",
      "193.80660463462024\n",
      "193.21490625725542\n",
      "200.68383093812017\n",
      "200.06249023742558\n",
      "199.44422779313518\n",
      "198.82907232092595\n",
      "198.21705274773913\n",
      "197.60819821049935\n",
      "197.00253805471644\n",
      "196.4001018329675\n",
      "195.80091930325557\n",
      "195.2050204272421\n",
      "194.61243536834948\n",
      "201.4671189052943\n",
      "200.84820138602188\n",
      "200.23236501624805\n",
      "199.61963831246663\n",
      "199.01004999748127\n",
      "198.40362899906847\n",
      "197.80040444852483\n",
      "197.2004056790959\n",
      "196.60366222428308\n",
      "196.01020381602586\n",
      "201.63581031156147\n",
      "201.02238681301145\n",
      "200.4120754844877\n",
      "199.80490484470096\n",
      "199.20090361240835\n",
      "198.600100704909\n",
      "198.00252523642217\n",
      "197.40820651634522\n",
      "201.20636172845033\n",
      "200.60159520801423\n",
      "200.0\n",
      "199.40160480798542\n",
      "198.80643852752857\n",
      "409.12223112414705\n",
      "408.09925263347395\n",
      "408.1225306204008\n",
      "408.1482573771448\n",
      "444.292696316291\n",
      "442.92888819764283\n",
      "443.42417615642023\n",
      "443.9211641721985\n",
      "441.5653971950248\n",
      "442.05995068542455\n",
      "442.55621111899444\n",
      "443.0541727599459\n",
      "443.5538298786293\n",
      "441.1915683691156\n",
      "441.688804476636\n",
      "442.18774293279546\n",
      "442.68837798162264\n",
      "440.82195952561165\n",
      "441.3218779983607\n",
      "439.95567958602373\n",
      "476.49554037787175\n",
      "477.3311219688069\n",
      "478.1673347270807\n",
      "475.94642555649057\n",
      "476.78296949450703\n",
      "477.6201419538334\n",
      "475.39877997319263\n",
      "476.2362858917829\n",
      "477.07441767506253\n",
      "474.8526087113769\n",
      "475.69107622489616\n",
      "476.53016693594543\n",
      "474.30791686414005\n",
      "475.14734556766706\n",
      "475.9873947910806\n",
      "472.92494119045995\n",
      "473.76470953417373\n",
      "474.60509900337144\n",
      "475.4461063043844\n",
      "472.3822604628586\n",
      "473.22299183365976\n",
      "474.0643416246364\n",
      "474.90630654898655\n",
      "471.8410749394334\n",
      "472.682768884164\n",
      "473.52507853333384\n",
      "474.3680006071236\n",
      "470.45934999742536\n",
      "471.3013897709193\n",
      "472.1440458165283\n",
      "472.9873148404722\n",
      "473.8311935700308\n",
      "471.60682777076073\n"
     ]
    }
   ],
   "source": [
    "ghost_positions, pacman_positions, _ = env.get_character_positions()\n",
    "if pacman_positions:\n",
    "    pacman_pos = pacman_positions[0]\n",
    "else:\n",
    "    pacman_pos = (0, 0) # Default position if not detected\n",
    "    \n",
    "if ghost_positions:\n",
    "    ghost_positions = ghost_positions\n",
    "else:\n",
    "    ghost_positions = (0,0)\n",
    "for ghost_pos in ghost_positions:\n",
    "    distance = env.calculate_distance(pacman_pos, ghost_pos)\n",
    "    print(distance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m total_reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[1;32m----> 8\u001b[0m     obs, reward, done, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maction_space\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m     total_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n\u001b[0;32m     10\u001b[0m     env\u001b[38;5;241m.\u001b[39mrender()\n",
      "Cell \u001b[1;32mIn[41], line 192\u001b[0m, in \u001b[0;36mPacMan.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    183\u001b[0m action_map \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;241m0\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m'\u001b[39m,   \u001b[38;5;66;03m# Move Left\u001b[39;00m\n\u001b[0;32m    185\u001b[0m     \u001b[38;5;241m1\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mright\u001b[39m\u001b[38;5;124m'\u001b[39m,  \u001b[38;5;66;03m# Move Right\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    188\u001b[0m     \u001b[38;5;241m4\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mno_op\u001b[39m\u001b[38;5;124m'\u001b[39m   \u001b[38;5;66;03m# No operation (do nothing)\u001b[39;00m\n\u001b[0;32m    189\u001b[0m }\n\u001b[0;32m    191\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m action \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m4\u001b[39m:\n\u001b[1;32m--> 192\u001b[0m     \u001b[43mpydirectinput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpress\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43maction\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    194\u001b[0m current_pellet_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread_pellet_count_from_file()\n\u001b[0;32m    195\u001b[0m pellet_reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_pellet_reward(current_pellet_count)\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\CondaEnvs\\.conda\\neuralnet\\lib\\site-packages\\pydirectinput\\__init__.py:243\u001b[0m, in \u001b[0;36m_genericPyDirectInputChecks.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    241\u001b[0m failSafeCheck()\n\u001b[0;32m    242\u001b[0m returnVal \u001b[38;5;241m=\u001b[39m wrappedFunction(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 243\u001b[0m \u001b[43m_handlePause\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfuncArgs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m_pause\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    244\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m returnVal\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\CondaEnvs\\.conda\\neuralnet\\lib\\site-packages\\pydirectinput\\__init__.py:232\u001b[0m, in \u001b[0;36m_handlePause\u001b[1;34m(_pause)\u001b[0m\n\u001b[0;32m    230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _pause:\n\u001b[0;32m    231\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(PAUSE, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(PAUSE, \u001b[38;5;28mfloat\u001b[39m)\n\u001b[1;32m--> 232\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPAUSE\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "env = PacMan()\n",
    "for episode in range(5):\n",
    "    obs = env.reset()\n",
    "    done =False\n",
    "    total_reward = 0\n",
    "    \n",
    "    while not done:\n",
    "        obs, reward, done, truncated, info = env.step(env.action_space.sample())\n",
    "        total_reward += reward\n",
    "        env.render()\n",
    "print(\"Reward: {total_reward}\")\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.patches as patches\n",
    "cap = mss()\n",
    "game_location = {'top':50, 'left':-2280, 'width':1400, 'height':1300}\n",
    "screen_capture = np.array(cap.grab(game_location))[:,:,:3]\n",
    "cv.imwrite('game_capture.png', screen_capture)\n",
    "\n",
    "ghost_template = cv.imread('ghost_template.png', 0)\n",
    "pacman_template_left = cv.imread('pacman_template_left.png', 0)\n",
    "pacman_template_right = cv.imread('pacman_template_right.png', 0)\n",
    "pacman_template_up = cv.imread('pacman_template_up.png', 0)\n",
    "pacman_template_down = cv.imread('pacman_template_down.png', 0)\n",
    "gray_screen = cv.cvtColor(screen_capture, cv.COLOR_BGR2GRAY)\n",
    "# Match the templates to find Pac-Man\n",
    "result_left = cv.matchTemplate(gray_screen, pacman_template_left, cv.TM_CCOEFF_NORMED)\n",
    "result_right = cv.matchTemplate(gray_screen, pacman_template_right, cv.TM_CCOEFF_NORMED)\n",
    "result_up = cv.matchTemplate(gray_screen, pacman_template_up, cv.TM_CCOEFF_NORMED)\n",
    "result_down = cv.matchTemplate(gray_screen, pacman_template_down, cv.TM_CCOEFF_NORMED)\n",
    "result_ghost = cv.matchTemplate(gray_screen, ghost_template, cv.TM_CCOEFF_NORMED)\n",
    "threshold = 0.5 # Adjust this value based on testing\n",
    "locations_left = np.where(result_left >= threshold)\n",
    "locations_right = np.where(result_right >= threshold)\n",
    "locations_up = np.where(result_up >= threshold)\n",
    "locations_down = np.where(result_down >= threshold)\n",
    "location_ghsot = np.where(result_ghost >= 0.5)\n",
    "combined_locations = list(zip(*locations_left[::-1])) + list(zip(*locations_right[::-1])) + list(zip(*locations_up[::-1])) + list(zip(*locations_down[::-1]))\n",
    "ghost_position = list(zip(*location_ghsot[::-1]))\n",
    "\n",
    "# Set up the Matplotlib figure\n",
    "fig, ax = plt.subplots(1)\n",
    "ax.imshow(screen_capture)\n",
    "\n",
    "# Draw rectangles around matched locations using Matplotlib patches\n",
    "for loc in combined_locations:\n",
    "    top_left = loc\n",
    "    bottom_right = (top_left[0] + pacman_template_right.shape[1], top_left[1] + pacman_template_right.shape[0])\n",
    "    \n",
    "    # Create a rectangle patch and add it to the plot\n",
    "    rect = patches.Rectangle(top_left, pacman_template_right.shape[1], pacman_template_right.shape[0], \n",
    "                             linewidth=2, edgecolor='b', facecolor='none')\n",
    "    ax.add_patch(rect)\n",
    "for loc in ghost_position:\n",
    "    top_left = loc\n",
    "    bottom_right = (top_left[0] + ghost_template.shape[1], top_left[1] + ghost_template.shape[0])\n",
    "    \n",
    "    # Create a rectangle patch and add it to the plot\n",
    "    rect = patches.Rectangle(top_left, ghost_template.shape[1], ghost_template.shape[0], \n",
    "                             linewidth=2, edgecolor='r', facecolor='none')\n",
    "    ax.add_patch(rect)\n",
    "# Display the result\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Designing DQN Model\n",
    "class DQN(nn.Module): # defines a new neural netwokr model that inherits from Pytorch's base class nn.module\n",
    "    def __init__(self, lr, input_dims, fc1_dims, fc2_dims, num_actions): \n",
    "        super(DQN, self).__init__() # calls the initializer of the parent class nn.module \n",
    "        self.input_dims = input_dims\n",
    "        self.fc1_dims = fc1_dims\n",
    "        self.fc2_dims = fc2_dims\n",
    "        self.conv1 = nn.Conv2d(self.input_dims, 32, kernel_size=8, stride=4) # convolutional layer with 32 filters, each of size 8 x8, applied with a stride of 4\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2) # convolutional layer with 64 filters, each of size 4 x 4, applied with a stride of 2\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1) # convolutional layer with 64 filters, each of size 3 x 3\n",
    "        self.fc_input_size = self._calculate_fc_input_size(self.input_dims)\n",
    "        self.fc1 = nn.Linear(self.fc_input_size, self.fc1_dims) # fully connected layer with 512 units\n",
    "        self.fc2 = nn.Linear(self.fc2_dims, num_actions) # final fully connected layer with output units equal to the number of possible actions\n",
    "        self.optimizer = AdamW(self.model.parameters(), lr=lr)\n",
    "        self.loss = nn.MSELoss()\n",
    "        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "        self.to(self.device)\n",
    "        \n",
    "    def _calculate_fc_input_size(self, input_dims):\n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.zeros(1, input_dims, 50, 80)\n",
    "            x = torch.relu(self.conv1(dummy_input))\n",
    "            x = torch.relu(self.conv2(x))\n",
    "            x = torch.relu(self.conv3(x))\n",
    "            \n",
    "            return x.view(1, -1).size(1)  # Flatten and get the size\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = torch.relu(self.conv3(x))\n",
    "        x = x.view(x.size(0), -1) # Flatten the output from conv layers\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        actions =  self.fc2(x)  # Output Q-Values for each action\n",
    "        \n",
    "        return actions\n",
    "    \n",
    "    \n",
    "# Creating DQN Agent\n",
    "class DQNAgent:       \n",
    "    def __init__(self, gamma, epsilon, lr, input_dims, batch_size, num_actions,\n",
    "                 max_mem_size=100000, eps_end=0.01, eps_dec=5e-4):\n",
    "        self.gamma = gamma # Determines the weighting of future rewards\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_min = eps_end\n",
    "        self.epsilon_decay = eps_dec\n",
    "        self.lr = lr\n",
    "        self.action_space = [i for i in range(num_actions)]\n",
    "        self.num_actions = num_actions\n",
    "        self.mem_size = max_mem_size\n",
    "        self.batch_size = batch_size\n",
    "        self.mem_cntr = 0\n",
    "        \n",
    "        self.Q_eval = DQN(self.lr, input_dims, fc1_dims=512, fc2_dims=512, num_actions=num_actions)\n",
    "        # self.memory = deque(maxlen=2000) rather than use this use this:\n",
    "        self.state_memory = np.zeros((self.mem_size, *input_dims), dtype=np.float32)\n",
    "        self.new_state_memory = np.zeros((self.mem_size, *input_dims), dtype=np.float32)\n",
    "        self.action_memory = np.zeros(self.mem_size, dtype=np.int32)\n",
    "        self.reward_memory = np.zeros(self.mem_size, dtype=np.float32)\n",
    "        self.terminal_memory = np.zeros(self.mem_size, dtype=np.bool)\n",
    "    \n",
    "    # Method for storing memory   \n",
    "    def store_transition(self, state, action, reward, state_, done):\n",
    "        index = self.mem_cntr % self.mem_size\n",
    "        self.state_memory[index] = state\n",
    "        self.new_state_memory[index] = state_\n",
    "        self.reward_memory[index] = reward\n",
    "        self.action_memory[index] = action\n",
    "        self.terminal_memory[index] = done\n",
    "        \n",
    "        self.mem_cntr += 1\n",
    "    \n",
    "    # Method for choosing an action\n",
    "    def choose_action(self, observation):\n",
    "        if np.random.random() > self.epsilon:\n",
    "            state = torch.tensor([observation]).to(self.Q_eval.device)\n",
    "            actions = self.Q_eval.forward(state)\n",
    "            action = torch.argmax(actions).item()\n",
    "        else:\n",
    "            action = np.random.choice(self.action_space)\n",
    "            \n",
    "        return action\n",
    "    \n",
    "    def learn(self):\n",
    "        if self.mem_cntr < self.batch_size:\n",
    "            return\n",
    "        self.Q_eval.optimizer.zero_grad()\n",
    "        \n",
    "        max_mem = min(self.mem_cntr, self.mem_size)\n",
    "        batch = np.random.choice(max_mem, self.batch_size, replace=False)\n",
    "        \n",
    "        batch_index = np.arange(self.batch_size, dtype=np.int32)\n",
    "        \n",
    "        state_batch = torch.tensor(self.state_memory[batch]).to(self.Q_eval.dvice)\n",
    "        new_state_batch = torch.tensor(self.new_state_memory[batch]).to(self.Q_eval.dvice)\n",
    "        reward_batch = torch.tensor(self.reard_memory[batch]).to(self.Q_eval.dvice)\n",
    "        terminal_batch = torch.tensor(self.terminal_memory[batch]).to(self.Q_eval.dvice)\n",
    "        \n",
    "        action_batch = self.action_memory[batch]\n",
    "        \n",
    "        q_eval = self.Q_eval.forward(state_batch)[batch_index, action_batch]\n",
    "        q_next = self.Q_eval.forward(new_state_batch)\n",
    "        q_next[terminal_batch] = 0.0\n",
    "        \n",
    "        q_target = reward_batch + self.gamma * torch.max(q_next, dim=1)[0]\n",
    "        \n",
    "        loss = self.Q_eval.loss(q_target, q_eval).to(self.Q_eval.device)\n",
    "        loss.backward()\n",
    "        self.Q_eval.optimizer.step()\n",
    "        \n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon = self.epsilon - self.epsilon_decay\n",
    "        else:\n",
    "            self.epsilon = self.epsilon_min\n",
    "        \n",
    "        \n",
    "        \n",
    "    # def remember(self, state, action, reward, next_state, done):\n",
    "    #     self.memory.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    # def act(self, state):\n",
    "    #     if np.random.rand() <= self.epsilon:\n",
    "    #         return random.randrange(self.num_actions)\n",
    "    #     state = torch.FloatTensor(state).unsqueezse(0)\n",
    "    #     q_values = self.model(state)\n",
    "    #     return torch.argmax(q_values[0]).item()\n",
    "    \n",
    "    # def replay(self, batch_size):\n",
    "    #     if len(self.memory) < batch_size:\n",
    "    #         return\n",
    "    #     minibatch = random.sample(self.memory. batch_size)\n",
    "    #     for state, action, reward, next_state, done in minibatch:\n",
    "    #         target = reward\n",
    "    #         if not done:\n",
    "    #             next_state = torch.FloatTensor(next_state).unsqueeze(0)\n",
    "    #             target += self.gamma * torch.max(self.model(next_state)).item()\n",
    "    #         state = torch.FloatTensor(state).unsqueeze(0)\n",
    "    #         target_f = self.model(state)\n",
    "    #         target_f[0][action] = target\n",
    "    #         self.optimizer.zero_grad()\n",
    "    #         loss = self.criterion(target_f, self.model(state))\n",
    "    #         loss.backward()\n",
    "    #         self.optimizer.step()\n",
    "        \n",
    "    #     if self.epsilon > self.epsilon_min:\n",
    "    #         self.epsilon *= self.epsilon_decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "num_episodes = 100\n",
    "batch_size = 32\n",
    "gamma = 0.99\n",
    "epsilon = 0.1 # Exploration rate\n",
    "buffer_capacity = 10000\n",
    "learning_rate = 1e-3    \n",
    "\n",
    "# Initialize environment and model\n",
    "env = PacMan()\n",
    "input_dims = env.observation_space.shape[0]\n",
    "num_actions = env.action_space.n\n",
    "\n",
    "model = GameNet(input_dims, num_actions)\n",
    "target_model = GameNet(input_dims, num_actions)\n",
    "target_model.load_state_dict(model.state_dict())\n",
    "optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.MSELoss()\n",
    "replay_buffer = ReplayBuffer(buffer_capacity) # Stores experiences (state, action, reward, next state, done) for training\n",
    "\n",
    "# Training function to choose an action using epsilon-greedy policy (Exploration vs exploitation)\n",
    "def select_action(state, epsilon):\n",
    "    if random.random() < epsilon:\n",
    "        return env.action_space.sample() # Random action (Exploration)\n",
    "    else:\n",
    "        state = torch.FloatTensor(state).unsqueeze(0) # Add batch dimension\n",
    "        with torch.no_grad():\n",
    "            q_values = model(state)\n",
    "        return q_values.argmax().item() # Action with highest Q-value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function to procvide interaction with the environment, \n",
    "def train_gamenet(env, model, target_model, optimizer, criterion, replay_buffer, num_episodes=10):\n",
    "    for episode in range(num_episodes):\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        \n",
    "        while not done:\n",
    "            action = select_action(state, epsilon)\n",
    "            next_state, reward, done, truncated, info = env.step(action)\n",
    "            total_reward += reward\n",
    "            \n",
    "            # Store experience in replay buffer\n",
    "            replay_buffer.push((state, action, reward, next_state, done))\n",
    "            state = next_state\n",
    "            \n",
    "            # Perform optimization step\n",
    "            if len(replay_buffer) >= batch_size:\n",
    "                batch = replay_buffer.sample(batch_size)\n",
    "                states, actions, rewards, next_states, dones = zip(*batch)\n",
    "                \n",
    "                states = torch.FloatTensor(np.array(states))\n",
    "                actions = torch.LongTensor(np.array(actions))\n",
    "                rewards = torch.FloatTensor(np.array(rewards))\n",
    "                next_states = torch.FloatTensor(np.array(next_states))\n",
    "                dones = torch.FloatTensor(np.array(dones))\n",
    "                \n",
    "                # Compute Q-values\n",
    "                q_values = model(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "                next_q_values = target_model(next_states).max(1)[0]\n",
    "                target_q_values = rewards + (gamma * next_q_values * (1 - dones))\n",
    "                \n",
    "                # Compute loss\n",
    "                loss = criterion(q_values, target_q_values)\n",
    "                \n",
    "                # Optimize the model\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "        # Print progress\n",
    "        print(f\"Episode {episode}: Total Reward = {total_reward}\")\n",
    "        \n",
    "        # Update target network\n",
    "        if episode % 100 == 0:\n",
    "            target_model.load_state_dict(model.state_dict())\n",
    "        \n",
    "train_gamenet(env, model, target_model, optimizer, criterion, replay_buffer, num_episodes)\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Total training time: {elapsed_time // 60:.0f} minutes, {elapsed_time % 60:.0f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 822,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dependencies\n",
    "import os\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common import env_checker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 824,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check environment for errors\n",
    "env_checker.check_env(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 823,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainAndLoggingCallback(BaseCallback):\n",
    "    \n",
    "    def __init__(self, check_freq, save_path, verbose=1):\n",
    "        super(TrainAndLoggingCallback, self).__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.save_path = save_path\n",
    "        \n",
    "    def _init_callback(self):\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "            \n",
    "    def _on_step(self):\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "            model_path = os.path.join(self.save_path, 'best_model_{}'.format(self.n_calls))\n",
    "            self.model.save(model_path)\n",
    "            \n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 825,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare directories to store files\n",
    "CHECKPOINT_DIR = './train/'\n",
    "LOG_DIR = './logs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 826,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = TrainAndLoggingCallback(check_freq=1000, save_path=CHECKPOINT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 827,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare model\n",
    "model = DQN('CnnPolicy', env, tensorboard_log=LOG_DIR, verbose=1, buffer_size=1200000, learning_starts=1000, exploration_initial_eps=1.0, exploration_final_eps=0.1, exploration_fraction=0.1, learning_rate = 0.01 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start learning\n",
    "model.learn(total_timesteps=5000, callback=callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "model.load(os.path.join('train', 'best_model_5000'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# play loaded model\n",
    "for episode in range(10):\n",
    "    obs, _ = env.reset()\n",
    "    done =False\n",
    "    total_reward = 0\n",
    "    \n",
    "    while not done:\n",
    "        action, _ = model.predict(obs)\n",
    "        obs, reward, done, _, info = env.step(int(action))\n",
    "        total_reward += reward\n",
    "    print(f'Total Reward for episode {episode} is {total_reward}')\n",
    "    time.sleep(2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dinoRL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
