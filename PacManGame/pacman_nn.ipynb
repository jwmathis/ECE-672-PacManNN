{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary Results\n",
    "\n",
    "**Resources & References**\n",
    "1. The Game: https://dinosaurgame.app/ \n",
    "2. Video Tutorial: https://www.youtube.com/watch?v=vahwuupy81A&t=5517s\n",
    "3. Stable-Baselines3 Documentations: https://stable-baselines3.readthedocs.io/en/master/\n",
    "4. Gymnasium Documentation: https://gymnasium.farama.org/tutorials/gymnasium_basics/environment_creation/\n",
    "5. PyTorch DQN Documentation for Gym Retro: https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Dependencies\n",
    "import torch # PyToch library for building and training neural networks\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "import numpy as np # for numerical calculations\n",
    "from collections import namedtuple, deque # provides useful data structures may not need\n",
    "import random # for random sampling \n",
    "from mss import mss # for grabbing a screen shot of a monitor \n",
    "import pydirectinput # for mouse and keyboard input on windows\n",
    "import cv2 # for image and video processing\n",
    "import pytesseract # OCR tool for reading text from images\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "from gymnasium import Env\n",
    "from gymnasium.spaces import Box, Discrete\n",
    "from gymnasium.utils import env_checker  # Import the environment checker\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PacMan(Env):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.observation_space = Box(low=0, high=255, shape=(1,50,80), dtype=np.uint8)\n",
    "        self.action_space = Discrete(5) # number of possible actions\n",
    "        \n",
    "        self.previous_lives = 2\n",
    "        self.current_lives = self.previous_lives\n",
    "        self.previous_score = 0\n",
    "        \n",
    "        self.pellet_address = 0x7268\n",
    "        self.file_path = \"pellet_count.txt\"\n",
    "        self.previous_pellet_count = self.read_pellet_count_from_file()\n",
    "        \n",
    "        self.cap = mss()\n",
    "        self.game_location = {'top':50, 'left':-2280, 'width':2000, 'height':1300} # defines game viewing location\n",
    "        #self.score_location = {'top':380, 'left':-920, 'width':600, 'height':80} # defines score location\n",
    "        self.done_location = {'top':520, 'left':-1800, 'width':450, 'height':70} # defines game over location\n",
    "        self.lives_location = {'top':1070, 'left':-902, 'width':600, 'height':200} # defines lives location\n",
    "        self.frame_stack = deque(maxlen=6) # stack frames to provide a sense of motion\n",
    "        #self.done_location = {'top':508, 'left':-1810, 'width':450, 'height':80}     \n",
    "    \n",
    "    # observation of the state of the environment\n",
    "    def get_observation(self):\n",
    "        # Get screen capture of game\n",
    "        raw = np.array(self.cap.grab(self.game_location))[:,:,:3]\n",
    "        # Grayscale\n",
    "        gray = cv2.cvtColor(raw, cv2.COLOR_BGR2GRAY)\n",
    "        # Resize\n",
    "        resized = cv2.resize(gray, (80,50))\n",
    "        # Add channels first\n",
    "        channel = np.reshape(resized, (1,50,80))\n",
    "        return channel\n",
    "    \n",
    "    def get_stacked_observation(self):\n",
    "        # stack the frames in the deque and convert to the required shape\n",
    "        return np.concatenate(list(self.frame_stack), axis=0)\n",
    "    \n",
    "    # get number of lives left\n",
    "    def get_lives(self):   \n",
    "        # Capture the area where the lives are displayed\n",
    "        lives_cap = np.array(self.cap.grab(self.lives_location))[:,:,:3]\n",
    "        # Convert to grayscale\n",
    "        lives_gray = cv2.cvtColor(lives_cap, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        # Load pacman life icon template\n",
    "        pacman_life_template = cv2.imread('pacman_life_icon.png', 0)\n",
    "        \n",
    "        # Perform template matching\n",
    "        result = cv2.matchTemplate(lives_gray, pacman_life_template, cv2.TM_CCORR_NORMED)\n",
    "        threshold = 0.8\n",
    "        locations = np.where(result >= threshold)\n",
    "        \n",
    "        lives_value = len(list(zip(*locations[::-1])))\n",
    "        \n",
    "        # Determine number of lives\n",
    "        if lives_value == 684:\n",
    "            num_lives = 2\n",
    "        elif lives_value == 344:\n",
    "            num_lives = 1\n",
    "        else:\n",
    "            num_lives = 0\n",
    "            \n",
    "        return num_lives\n",
    "    \n",
    "    # Get game over\n",
    "    def get_done(self):\n",
    "        # Get the number of lives left \n",
    "        num_lives = self.get_lives()\n",
    "        return num_lives == 0 # return bool\n",
    "    \n",
    "    def read_pellet_count_from_file(self):\n",
    "        try:\n",
    "            with open(self.file_path, \"r\") as file:\n",
    "                return int(file.read().strip())\n",
    "        except (FileNotFoundError, ValueError):\n",
    "            return 0\n",
    "        \n",
    "    # Resets the environment to its initial state\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        # restart the game\n",
    "        pydirectinput.click(x=-890, y=374) # select game window\n",
    "        pydirectinput.press('f1') # Start state 1 save\n",
    "        \n",
    "        # reset pellet count\n",
    "        self.previous_pellet_count = self.read_pellet_count_from_file()\n",
    "        \n",
    "        # reset frame stack\n",
    "        self.frame_stack.clear()\n",
    "        for _ in range(6):\n",
    "            initial_frame = self.get_observation()\n",
    "            self.frame_stack.append(initial_frame)\n",
    "            \n",
    "        return self.get_stacked_observation(), {}\n",
    "    \n",
    "    def get_ghosts_positions(self):\n",
    "        ghosts_cap = np.array(self.cap.grab(self.ghosts_location))[:] \n",
    "        \n",
    "    \n",
    "    # Reward for eating pellets\n",
    "    def get_pellet_reward(self, current_pellet_count):\n",
    "        if current_pellet_count < self.previous_pellet_count:\n",
    "            reward = 30 \n",
    "            self.previous_pellet_count = current_pellet_count\n",
    "        else:\n",
    "            reward = 0    \n",
    "        return reward\n",
    "    \n",
    "    # def get_score(self):\n",
    "    #     score_cap = np.array(self.cap.grab(self.score_location))[:,:,:3]\n",
    "    #     score_gray =cv2.cvtColor(score_cap, cv2.COLOR_BGR2GRAY)\n",
    "    #     _, score_thresh = cv2.threshold(score_gray, 150, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    #     # Apply OCR on the processed image\n",
    "    #     score_text = pytesseract.image_to_string(score_thresh, config='digits')\n",
    "        \n",
    "    #     try:\n",
    "    #         score_value = int(score_text.strip())\n",
    "    #     except ValueError:\n",
    "    #         score_value = self.previous_score\n",
    "            \n",
    "    #     return score_cap, score_text\n",
    "    \n",
    "    # Action that is called to do something in the game\n",
    "    def step(self, action):\n",
    "        action_map = {\n",
    "            0: 'left',   # Move Left\n",
    "            1: 'right',  # Move Right\n",
    "            2: 'up',     # Move Up\n",
    "            3: 'down',   # Move Down\n",
    "            4: 'no_op'   # No operation (do nothing)\n",
    "        }\n",
    "        \n",
    "        if action != 4:\n",
    "            pydirectinput.press(action_map[action])\n",
    "            \n",
    "        current_pellet_count = self.read_pellet_count_from_file()\n",
    "        pellet_reward = self.get_pellet_reward(current_pellet_count)\n",
    "        \n",
    "        current_lives = self.get_lives()\n",
    "        life_penalty = 0\n",
    "        # Penalize only when a life is lost (and only once per life loss)\n",
    "        if current_lives < self.previous_lives:\n",
    "            life_penalty -= 100\n",
    "            self.previous_lives = current_lives # update previous lives \n",
    "            \n",
    "        reward = pellet_reward + life_penalty\n",
    "        \n",
    "        # Penalize heavily if all lives are lost\n",
    "        done = self.get_done()\n",
    "        # end_game_penalty = 0\n",
    "        # if done:\n",
    "        #     end_game_penalty -= 500\n",
    "        # else: \n",
    "        #     end_game_penalty -= 0\n",
    "            \n",
    "    \n",
    "        # Get the next observation\n",
    "        new_frame = self.get_observation()\n",
    "        self.frame_stack.append(new_frame)\n",
    "        stacked_observation = self.get_stacked_observation()\n",
    "        \n",
    "        return stacked_observation, reward, done, False, {}\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Designing convolutional neural network\n",
    "class GameNet(nn.Module): # defines a new neural netwokr model that inherits from Pytorch's base class nn.module\n",
    "    def __init__(self, input_channels, num_actions): \n",
    "        super(GameNet, self).__init__() # calls the initializer of the parent class nn.module \n",
    "        self.conv1 = nn.Conv2d(input_channels, 32, kernel_size=8, stride=4) # convolutional layer with 32 filters, each of size 8 x8, applied with a stride of 4\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2) # convolutional layer with 64 filters, each of size 4 x 4, applied with a stride of 2\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1) # convolutional layer with 64 filters, each of size 3 x 3\n",
    "        \n",
    "        self._calculate_fc_input_size(input_channels)\n",
    "        \n",
    "        self.fc1 = nn.Linear(self.fc_input_size, 512) # fully connected layer with 512 units\n",
    "        self.fc2 = nn.Linear(512, num_actions) # final fully connected layer with output units equal to the number of possible actions\n",
    "        \n",
    "    def _calculate_fc_input_size(self, input_channels):\n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.zeros(1, input_channels, 50, 80)\n",
    "            x = torch.relu(self.conv1(dummy_input))\n",
    "            x = torch.relu(self.conv2(x))\n",
    "            x = torch.relu(self.conv3(x))\n",
    "            self.fc_input_size = x.view(1, -1).size(1)  # Flatten and get the size\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = torch.relu(self.conv3(x))\n",
    "        x = x.view(x.size(0), -1) # Flatten the output from conv layers\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "# Define a replay buffer\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    \n",
    "    def push(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.buffer, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "num_episodes = 100\n",
    "batch_size = 32\n",
    "gamma = 0.99\n",
    "epsilon = 0.1 # Exploration rate\n",
    "buffer_capacity = 10000\n",
    "learning_rate = 1e-3    \n",
    "\n",
    "# Initialize environment and model\n",
    "env = PacMan()\n",
    "input_channels = env.observation_space.shape[0]\n",
    "num_actions = env.action_space.n\n",
    "\n",
    "model = GameNet(input_channels, num_actions)\n",
    "target_model = GameNet(input_channels, num_actions)\n",
    "target_model.load_state_dict(model.state_dict())\n",
    "optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.MSELoss()\n",
    "replay_buffer = ReplayBuffer(buffer_capacity) # Stores experiences (state, action, reward, next state, done) for training\n",
    "\n",
    "# Training function to choose an action using epsilon-greedy policy (Exploration vs exploitation)\n",
    "def select_action(state, epsilon):\n",
    "    if random.random() < epsilon:\n",
    "        return env.action_space.sample() # Random action (Exploration)\n",
    "    else:\n",
    "        state = torch.FloatTensor(state).unsqueeze(0) # Add batch dimension\n",
    "        with torch.no_grad():\n",
    "            q_values = model(state)\n",
    "        return q_values.argmax().item() # Action with highest Q-value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function to procvide interaction with the environment, \n",
    "def train_gamenet(env, model, target_model, optimizer, criterion, replay_buffer, num_episodes=10):\n",
    "    for episode in range(num_episodes):\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        \n",
    "        while not done:\n",
    "            action = select_action(state, epsilon)\n",
    "            next_state, reward, done, truncated, info = env.step(action)\n",
    "            total_reward += reward\n",
    "            \n",
    "            # Store experience in replay buffer\n",
    "            replay_buffer.push((state, action, reward, next_state, done))\n",
    "            state = next_state\n",
    "            \n",
    "            # Perform optimization step\n",
    "            if len(replay_buffer) >= batch_size:\n",
    "                batch = replay_buffer.sample(batch_size)\n",
    "                states, actions, rewards, next_states, dones = zip(*batch)\n",
    "                \n",
    "                states = torch.FloatTensor(np.array(states))\n",
    "                actions = torch.LongTensor(np.array(actions))\n",
    "                rewards = torch.FloatTensor(np.array(rewards))\n",
    "                next_states = torch.FloatTensor(np.array(next_states))\n",
    "                dones = torch.FloatTensor(np.array(dones))\n",
    "                \n",
    "                # Compute Q-values\n",
    "                q_values = model(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "                next_q_values = target_model(next_states).max(1)[0]\n",
    "                target_q_values = rewards + (gamma * next_q_values * (1 - dones))\n",
    "                \n",
    "                # Compute loss\n",
    "                loss = criterion(q_values, target_q_values)\n",
    "                \n",
    "                # Optimize the model\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "        # Print progress\n",
    "        print(f\"Episode {episode}: Total Reward = {total_reward}\")\n",
    "        \n",
    "        # Update target network\n",
    "        if episode % 100 == 0:\n",
    "            target_model.load_state_dict(model.state_dict())\n",
    "        \n",
    "train_gamenet(env, model, target_model, optimizer, criterion, replay_buffer, num_episodes)\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Total training time: {elapsed_time // 60:.0f} minutes, {elapsed_time % 60:.0f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 821,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = PacMan()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_pellet_count, obs, reward, done, truncated, info = env.step(env.action_space.sample())\n",
    "print(current_pellet_count)\n",
    "print(reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pellet_count = env.read_pellet_count_from_file()\n",
    "pellet_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_pellet_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel = env.get_observation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(cv2.cvtColor(env.get_observation()[0], cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = PacMan()\n",
    "score_cap, score_text = env.get_score()\n",
    "plt.imshow(score_cap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lives_cap, num_lives = env.get_lives()\n",
    "num_lives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(lives_cap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "done = env.get_done()\n",
    "done\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Game loop\n",
    "obs = env.reset()\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    #env.render()  # Render the game screen\n",
    "    action = env.action_space.sample()  # Sample random action\n",
    "    obs, reward, done, truncated, info = env.step(action)  # Take the step\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):  # Graceful exit if 'q' is pressed\n",
    "        done = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Play 10 games\n",
    "for episode in range(2):\n",
    "    obs = env.reset()\n",
    "    done =False\n",
    "    total_reward = 0\n",
    "    \n",
    "    while not done:\n",
    "        obs, reward, done, truncated, info = env.step(env.action_space.sample())\n",
    "        total_reward += reward\n",
    "    print(f'Total Reward for episode {episode} is {total_reward}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 822,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dependencies\n",
    "import os\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common import env_checker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 824,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check environment for errors\n",
    "env_checker.check_env(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 823,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainAndLoggingCallback(BaseCallback):\n",
    "    \n",
    "    def __init__(self, check_freq, save_path, verbose=1):\n",
    "        super(TrainAndLoggingCallback, self).__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.save_path = save_path\n",
    "        \n",
    "    def _init_callback(self):\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "            \n",
    "    def _on_step(self):\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "            model_path = os.path.join(self.save_path, 'best_model_{}'.format(self.n_calls))\n",
    "            self.model.save(model_path)\n",
    "            \n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 825,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare directories to store files\n",
    "CHECKPOINT_DIR = './train/'\n",
    "LOG_DIR = './logs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 826,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = TrainAndLoggingCallback(check_freq=1000, save_path=CHECKPOINT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 827,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare model\n",
    "model = DQN('CnnPolicy', env, tensorboard_log=LOG_DIR, verbose=1, buffer_size=1200000, learning_starts=1000, exploration_initial_eps=1.0, exploration_final_eps=0.1, exploration_fraction=0.1, learning_rate = 0.01 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start learning\n",
    "model.learn(total_timesteps=5000, callback=callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "model.load(os.path.join('train', 'best_model_5000'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# play loaded model\n",
    "for episode in range(10):\n",
    "    obs, _ = env.reset()\n",
    "    done =False\n",
    "    total_reward = 0\n",
    "    \n",
    "    while not done:\n",
    "        action, _ = model.predict(obs)\n",
    "        obs, reward, done, _, info = env.step(int(action))\n",
    "        total_reward += reward\n",
    "    print(f'Total Reward for episode {episode} is {total_reward}')\n",
    "    time.sleep(2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dinoRL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
